@article{Anatolyev2018,
   abstract = {This paper re-examines the problem of estimating risk premia in linear factor pricing models. Typically, the data used in the empirical literature are characterized by weakness of some pricing factors, strong cross-sectional dependence in the errors, and (moderately) high cross-sectional dimensionality. Using an asymptotic framework where the number of assets/portfolios grows with the time span of the data while the risk exposures of weak factors are local-to-zero, we show that the conventional two-pass estimation procedure delivers inconsistent estimates of the risk premia. We propose a new estimation procedure based on sample-splitting instrumental variables regression. The proposed estimator of risk premia is robust to weak included factors and to the presence of strong unaccounted cross-sectional error dependence. We derive the many-asset weak factor asymptotic distribution of the proposed estimator, show how to construct its standard errors, verify its performance in simulations, and revisit some empirical studies.},
   author = {Stanislav Anatolyev and Anna Mikusheva},
   journal = {CESifo Working Paper Series},
   month = {7},
   title = {Factor models with many assets: strong factors, weak factors, and the two-pass procedure},
   url = {http://arxiv.org/abs/1807.04094},
   year = {2018},
}
@article{Harvey2019,
   abstract = {The rate of factor production in the academic research is out of control. We document over 400 factors published in top journals. Surely, many of them are false. We explore the incentives that lead to factor mining and explore reasons why many of the factors are simply lucky findings. The backtested results published in academic outlets are routinely cited to support commercial products. As a consequence, investors develop exaggerated expectations based on inflated backtested results and are then disappointed by the live trading experience. We provide a comprehensive census of factors published in top academic journals through January 2019. We also offer a link to a Google sheet that has detailed information on each factor, including citation information and download links. Finally, we propose a citizen science project that allows researchers to add to our database both published papers as well as working papers.},
   author = {Campbell R. Harvey and Yan Liu},
   doi = {10.2139/ssrn.3341728},
   journal = {SSRN Electronic Journal},
   keywords = {Backtesting,Data Mining,Factor Investing,Momentum,Multiple Testing,Overfitting,Value Investing},
   month = {3},
   publisher = {Elsevier BV},
   title = {A Census of the Factor Zoo},
   year = {2019},
}
@working_paper{Feng2019,
   author = {Guanhao Feng and Stefano Giglio and Dacheng Xiu},
   city = {Cambridge, MA},
   doi = {10.3386/w25481},
   institution = {National Bureau of Economic Research},
   month = {1},
   title = {Taming the Factor Zoo: A Test of New Factors},
   url = {http://www.nber.org/papers/w25481.pdf},
   year = {2019},
}
@article{Bailey2016,
   abstract = {This paper provides a characterisation of the degree of cross-sectional dependence in a two dimensional array, \{xit,i = 1,2,..N;t = 1,2,..,T\} in terms of the rate at which the variance of the cross-sectional average of the observed data varies with N. Under certain conditions this is equivalent to the rate at which the largest eigenvalue of the covariance matrix of xt=(x1t,x2t,..,xNt)′ rises with N. We represent the degree of cross-sectional dependence by α, which we refer to as the ‘exponent of cross-sectional dependence’, and define it by the standard deviation, Std((Formula presented.))= O (Nα-1), where (Formula presented.) is a simple cross-sectional average of xit. We propose bias corrected estimators, derive their asymptotic properties for α > 1/2 and consider a number of extensions. We include a detailed Monte Carlo simulation study supporting the theoretical results. We also provide a number of empirical applications investigating the degree of inter-linkages of real and financial variables in the global economy. Copyright © 2015 John Wiley & Sons, Ltd.},
   author = {Natalia Bailey and George Kapetanios and M. Hashem Pesaran},
   doi = {10.1002/jae.2476},
   issn = {08837252},
   issue = {6},
   journal = {Journal of Applied Econometrics},
   month = {9},
   pages = {929-960},
   publisher = {John Wiley and Sons Ltd},
   title = {Exponent of Cross-Sectional Dependence: Estimation and Inference},
   volume = {31},
   url = {http://doi.wiley.com/10.1002/jae.2476},
   year = {2016},
}
@working_paper{Bailey2020,
   abstract = {This paper proposes an estimator of factor strength and establishes its consistency and asymp-totic distribution. The proposed estimator is based on the number of statistically significant factor loadings, taking account of the multiple testing problem. We focus on the case where the factors are observed which is of primary interest in many applications in macroeconomics and finance. We also consider using cross section averages as a proxy in the case of unobserved common factors. We face a fundamental factor identification issue when there are more than one unobserved common factors. We investigate the small sample properties of the proposed estimator by means of Monte Carlo experiments under a variety of scenarios. In general, we find that the estimator, and the associated inference, perform well. The test is conservative under the null hypothesis, but, nevertheless, has excellent power properties, especially when the factor strength is sufficiently high. Application of the proposed estimation strategy to factor models of asset returns shows that out of 146 factors recently considered in the finance literature, only the market factor is truly strong, while all other factors are at best semi-strong, with their strength varying considerably over time. Similarly, we only find evidence of semi-strong factors in an updated version of the Stock and Watson (2012) macroeconomic dataset.},
   author = {Natalia Bailey and George Kapetanios and M. Hashem Pesaran},
   keywords = {E20,Factor models,G20,cross-sectional depen-dence,factor strength,market factor JEL Classifications: C38,measures of pervasiveness},
   title = {Measurement of Factor Strength: Theory and Practice},
   year = {2020},
}
@article{Sarafidis2012,
   abstract = {This article provides an overview of the existing literature on panel data models with error cross-sectional dependence (CSD). We distinguish between weak and strong CSD and link these concepts to the spatial and factor structure approaches. We consider estimation under strong and weak exogeneity of the regressors for both T fixed and T large cases. Available tests for CSD and methods for determining the number of factors are discussed in detail. The finite-sample properties of some estimators and statistics are investigated using Monte Carlo experiments. © 2012 Copyright Taylor and Francis Group, LLC.},
   author = {Vasilis Sarafidis and Tom Wansbeek},
   doi = {10.1080/07474938.2011.611458},
   issn = {0747-4938},
   issue = {5},
   journal = {Econometric Reviews},
   keywords = {Cross-sectional dependence,Factor structure,Panel data,Spatial dependence,Strong/Weak exogeneity},
   month = {9},
   pages = {483-531},
   publisher = { Taylor & Francis Group },
   title = {Cross-Sectional Dependence in Panel Data Analysis},
   volume = {31},
   url = {https://www.tandfonline.com/doi/full/10.1080/07474938.2011.611458},
   year = {2012},
}
@article{Chamberlain1983,
   abstract = {We present a definition of factor structure that is less restrictive than the one typically used in arbitrage pricing models. Our factor structure restrictions build on the following intuitive distinctions between factor variance and idiosyncratic variance: (i) A well-diversified portfolio contains only factor variance. (ii) If a portfolio is uncorrelated with the well-diversified portfolios, then it contains only idiosyncratic variance; so if a sequence of such portfolios becomes well-diversified, the limiting variance should be zero. Our factor structure restrictions imply Ross' [5] arbitrage pricing formula. We obtain upper and lower bounds on the approximation error in that formula; these bounds may be useful in empirical work. They imply that arbitrage pricing is exact if and only if there is a risky, well-diversified portfolio on the mean-variance frontier. If all mean-variance efficient portfolios are well-diversified, then the well-diversified portfolios provide mutual fund separation. Our factor structure restrictions are satisfied (with K factors) if and only if the covariance matrix of asset returns has only K unbounded eigenvalues as the number of assets increases.},
   author = {Gary Chamberlain},
   doi = {10.2307/1912276},
   issn = {00129682},
   issue = {5},
   journal = {Econometrica},
   month = {9},
   pages = {1305},
   publisher = {JSTOR},
   title = {Funds, Factors, and Diversification in Arbitrage Pricing Models},
   volume = {51},
   year = {1983},
}
@article{Basak2018,
   abstract = {We provide various norm-based definitions of different types of cross-sectional dependence and the relations between them. These definitions facilitate to comprehend and to characterize the various forms of cross-sectional dependence, such as strong, semi-strong, and weak dependence. Then we examine the asymptotic properties of parameter estimators both for fixed (within) effect estimator and random effect (pooled) estimator for linear panel data models incorporating various forms of cross-sectional dependence. The asymptotic properties are also derived when both cross-sectional and temporal dependence are present. Subsequently, we develop consistent and robust standard error of the parameter estimators both for fixed effect and random effect model separately. Robust standard errors are developed (i) for pure cross-sectional dependence; and (ii) also for cross-sectional and time series dependence. Under strong or semi-strong cross-sectional dependence, it is established that when the time dependence comes through the idiosyncratic errors, such time dependence does not have any influence in the asymptotic variance of $(\hat\{\beta\}_\{FE/RE\}). $ Hence, it is argued that in estimating $Var(\hat\{\beta\}_\{FE/RE\}),$ Newey-West kind of correction injects bias in the variance estimate. Furthermore, this article lay down conditions under which $t$, $F$ and the $Wald$ statistics based on the robust covariance matrix estimator give valid inference.},
   author = {Gopal K Basak and Samarjit Das},
   month = {4},
   title = {Understanding Cross-sectional Dependence in Panel Data},
   url = {http://arxiv.org/abs/1804.08326},
   year = {2018},
}
@article{Chen2011,
   abstract = {A new factor model consisting of the market factor, an investment factor, and a return-on-equity factor is a good start to understanding the cross-section of expected stock returns. Firms will invest a lot when their profitability is high and the cost of capital is low. As such, controlling for profitability, investment should be negatively correlated with expected returns, and controlling for investment, profitability should be positively correlated with expected returns. The new three-factor model reduces the magnitude of the abnormal returns of a wide range of anomalies-based trading strategies, often to insignificance. The model's performance, combined with its economic intuition, suggests that it can be used to obtain expected return estimates in practice.},
   author = {Long Chen and Robert Novy-Marx and Lu Zhang},
   doi = {10.2139/ssrn.1418117},
   journal = {SSRN Electronic Journal},
   keywords = {Alphas,Anomalies,Asset Pricing Tests,Factor Regressions},
   month = {12},
   publisher = {Elsevier BV},
   title = {An Alternative Three-Factor Model},
   year = {2011},
}
@article{Duffie1996,
   abstract = {This paper presents a consistent and arbitrage-free multifactor model of the term structure of interest rates in which yields at selected fixed maturities follow a parametric multivariate Markov diffusion process with "stochastic volatility." The yield of any zero-coupon bond is taken to be a maturity-dependent affine combination of the selected "basis" set of yields. We provide necessary and sufficient conditions on the stochastic model for this affine representation. We include numerical techniques for solving the model, as well as numerical techniques for calculating the prices of term-structure derivative prices. The case of jump diffusions is also considered.},
   author = {Darrell Duffie and Rui Kan},
   doi = {10.1111/j.1467-9965.1996.tb00123.x},
   issn = {0960-1627},
   issue = {4},
   journal = {Mathematical Finance},
   month = {10},
   pages = {379-406},
   publisher = {Blackwell Publishing Inc.},
   title = {A YIELD-FACTOR MODEL OF INTEREST RATES},
   volume = {6},
   url = {http://doi.wiley.com/10.1111/j.1467-9965.1996.tb00123.x},
   year = {1996},
}
@article{Harvey2014,
   abstract = {We propose a new regression method to select amongst a large group of candidate factors - many of which might be the result of data mining - that purport to explain the cross-section of expected returns. The method is robust to general distributional characteristics of both factor and asset returns. We allow for the possibility of time-series as well as cross-sectional dependence. The technique accommodates a wide range of test statistics such as t-ratios. While our main application focuses on asset pricing, the method can be applied in any situation where regression analysis is used in the presence of multiple testing. This includes, for example, the evaluation of investment manager performance as well as time-series prediction of asset returns.},
   author = {Campbell R. Harvey and Yan Liu},
   doi = {10.2139/ssrn.2528780},
   journal = {SSRN Electronic Journal},
   keywords = {Bootstrap,Data mining,Factors,Fama-MacBeth,GRS,Multiple testing,Orthogonalization,Performance evaluation,Predictive regressions,Return prediction,Variable selection},
   month = {11},
   publisher = {Elsevier BV},
   title = {Lucky Factors},
   year = {2014},
}
@article{Harvey2015,
   abstract = {Hundreds of papers and factors attempt to explain the cross-section of expected returns. Given this extensive data mining, it does not make sense to use the usual criteria for establishing significance. Which hurdle should be used for current research? Our paper introduces a new multiple testing framework and provides historical cutoffs from the first empirical tests in 1967 to today. A new factor needs to clear a much higher hurdle, with a t-statistic greater than 3.0. We argue that most claimed research findings in financial economics are likely false.Received October 22, 2014; accepted June 15, 2015 by Editor Andrew Karolyi.},
   author = {Campbell R Harvey and Yan Liu and Heqing Zhu},
   doi = {10.1093/rfs/hhv059},
   issn = {0893-9454},
   issue = {1},
   journal = {The Review of Financial Studies},
   month = {10},
   pages = {5-68},
   title = {… and the Cross-Section of Expected Returns},
   volume = {29},
   url = {https://doi.org/10.1093/rfs/hhv059},
   year = {2015},
}
@article{Bailey2019,
   abstract = {This paper proposes a regularisation method for the estimation of large covariance matrices that uses insights from the multiple testing (MT) literature. The approach tests the statistical significance of individual pair-wise correlations and sets to zero those elements that are not statistically significant, taking account of the multiple testing nature of the problem. The effective p-values of the tests are set as a decreasing function of N (the cross section dimension), the rate of which is governed by the nature of dependence of the underlying observations, and the relative expansion rates of N and T (the time dimension). In this respect, the method specifies the appropriate thresholding parameter to be used under Gaussian and non-Gaussian settings. The MT estimator of the sample correlation matrix is shown to be consistent in the spectral and Frobenius norms, and in terms of support recovery, so long as the true covariance matrix is sparse. The performance of the proposed MT estimator is compared to a number of other estimators in the literature using Monte Carlo experiments. It is shown that the MT estimator performs well and tends to outperform the other estimators, particularly when N is larger than T.},
   author = {Natalia Bailey and M. Hashem Pesaran and L. Vanessa Smith},
   doi = {10.1016/j.jeconom.2018.10.006},
   issn = {18726895},
   issue = {2},
   journal = {Journal of Econometrics},
   keywords = {High-dimensional data,Multiple testing,Non-Gaussian observations,Shrinkage,Sparsity,Thresholding},
   month = {2},
   pages = {507-534},
   publisher = {Elsevier Ltd},
   title = {A multiple testing approach to the regularisation of large sample correlation matrices},
   volume = {208},
   year = {2019},
}
@article{Chudik2018,
   abstract = {© 2018 The Econometric Society This paper provides an alternative approach to penalized regression for model selection in the context of high-dimensional linear regressions where the number of covariates is large, often much larger than the number of available observations. We consider the statistical significance of individual covariates one at a time, while taking full account of the multiple testing nature of the inferential problem involved. We refer to the proposed method as One Covariate at a Time Multiple Testing (OCMT) procedure, and use ideas from the multiple testing literature to control the probability of selecting the approximating model, the false positive rate, and the false discovery rate. OCMT is easy to interpret, relates to classical statistical analysis, is valid under general assumptions, is faster to compute, and performs well in small samples. The usefulness of OCMT is also illustrated by an empirical application to forecasting U.S. output growth and inflation.},
   author = {A. Chudik and G. Kapetanios and M. Hashem Pesaran},
   doi = {10.3982/ecta14176},
   issn = {0012-9682},
   issue = {4},
   journal = {Econometrica},
   keywords = {Monte Carlo experiments,One covariate at a time,boosting,high dimensionality,model selection,multiple testing,penalized regressions},
   month = {7},
   pages = {1479-1512},
   publisher = {The Econometric Society},
   title = {A One Covariate at a Time, Multiple Testing Approach to Variable Selection in High-Dimensional Linear Regression Models},
   volume = {86},
   year = {2018},
}
@article{Pukthuanthong2019,
   abstract = {We propose a protocol for identifying genuine risk factors. A genuine risk factor must be related to the covariance matrix of returns, must be priced in the cross-section of returns, and should yield a reward-to-risk ratio that is reasonable enough to be consistent with risk pricing. A market factor, a profitability factor, and traded versions of macroeconomic factors pass our protocol, but many characteristic-based factors do not. Several of the underlying characteristics, however, do command premiums in the cross-section.},
   author = {Kuntara Pukthuanthong and Richard Roll and Avanidhar Subrahmanyam},
   doi = {10.1093/rfs/hhy093},
   issn = {14657368},
   issue = {4},
   journal = {Review of Financial Studies},
   month = {8},
   pages = {1573-1607},
   title = {A protocol for factor identification},
   volume = {32},
   url = {https://doi.org/10.1093/rfs/hhy093},
   year = {2019},
}
@article{Uematsu2019,
   abstract = {In this paper, we propose a novel consistent estimation method for the approximate factor model of Chamberlain and Rothschild (1983), with large cross-sectional and time-series dimensions (N and T, respectively). Their model assumes that the r (≪N) largest eigenvalues of data covariance matrix grow as N rises without specifying each diverging rate. This is weaker than the typical assumption on the recent factor models, in which all the r largest eigenvalues diverge proportionally to N, and is frequently referred to as the weak factor models. We extend the sparse orthogonal factor regression (SOFAR) proposed by Uematsu et al. (2019) to consider consistent estimation of the weak factors structure, where the k-th largest eigenvalue grows proportionally to N^\{α_\{k\}\} with some unknown exponents 0},
   author = {Yoshimasa Uematsu and Takashi Yamagata},
   journal = {ISER Discussion Paper},
   publisher = {Institute of Social and Economic Research, Osaka University},
   title = {Estimation of Weak Factor Models},
   year = {2019},
}
@article{Pesaran2019,
   abstract = {In this paper we are concerned with the role of factor strength and pricing errors in asset pricing models, and their implications for identification and estimation of risk premia. We establish an explicit relationship between the pricing errors and the presence of weak factors that are correlated with stochastic discount factor. We introduce a measure of factor strength, and distinguish between observed factors and unobserved factors. We show that unobserved factors matter for pricing if they are correlated with the discount factor, and relate the strength of the weak factors to the strength (pervasiveness) of non-zero pricing errors. We then show, that even when the factor loadings are known, the risk premia of a factor can be consistently estimated only if it is strong and if the pricing errors are weak. Similar results hold when factor loadings are estimated, irrespective of whether individual returns or portfolio returns are used. We derive distributional results for two pass estimators of risk premia, allowing for non-zero pricing errors. We show that for inference on risk premia the pricing errors must be sufficiently weak. We consider both when n (the number of securities) is large and T (the number of time periods) is short, and the case of large n and T. Large n is required for consistent estimation of risk premia, whereas the choice of short T is intended to reduce the possibility of time variations in the factor loadings. We provide monthly rolling estimates of the factor strengths for the three Fama-French factors over the period 1989-2018.},
   author = {M. Hashem Pesaran and Ron P. Smith},
   journal = {CESifo Working Paper Series},
   keywords = {APT,Fama-French factors,arbitrage pricing theory,factor strength,identification of risk premia,two-pass regressions},
   publisher = {CESifo Group Munich},
   title = {The Role of Factor Strength and Pricing Errors for Estimation and Inference in Asset Pricing Models},
   year = {2019},
}
@article{MCLEAN2016,
   abstract = {We study the out-of-sample and post-publication return predictability of 97 variables shown to predict cross-sectional stock returns. Portfolio returns are 26% lower out-of-sample and 58% lower post-publication. The out-of-sample decline is an upper bound estimate of data mining effects. We estimate a 32% (58%-26%) lower return from publication-informed trading. Post-publication declines are greater for predictors with higher in-sample returns, and returns are higher for portfolios concentrated in stocks with high idiosyncratic risk and low liquidity. Predictor portfolios exhibit post-publication increases in correlations with other published-predictor portfolios. Our findings suggest that investors learn about mispricing from academic publications.},
   author = {R. DAVID MCLEAN and JEFFREY PONTIFF},
   doi = {10.1111/jofi.12365},
   issn = {00221082},
   issue = {1},
   journal = {The Journal of Finance},
   month = {2},
   pages = {5-32},
   publisher = {Blackwell Publishing Ltd},
   title = {Does Academic Research Destroy Stock Return Predictability?},
   volume = {71},
   url = {http://doi.wiley.com/10.1111/jofi.12365},
   year = {2016},
}
@article{Harvey2017,
   abstract = {Investors make two types of mistakes. First, they erroneously allocate to an asset manager (or a “smart” beta) that underperforms because the asset manager lacks skill. Second, investors might miss out allocating to a good manager. The first mistake is difficult to deal with given there are thousands of managers and many look good purely by luck. We introduce a new technique that optimizes the threshold for a prespecified false discovery rate (i.e., chance of the first mistake), at say 5%. Our method also allows for heterogeneous false discoveries – we should not treat all bad managers the same because some are really, really bad. Next, we focus on the second type of error where investors miss out on good managers. It is routine to ignore this type of mistake. Our results show that current research methods have little or no power to detect good managers. Finally, our method allows for the asymmetric treatment of false discoveries and misses – generally, investing in a bad manager is more costly than missing a good manager. We also offer a way to select managers whereby the investor can prespecify the ratio of false discoveries to misses to accommodate these differential costs. For instance, we can accommodate a decision rule whereby the investor is willing to miss ten good managers to avoid the mistake of selecting one bad manager.},
   author = {Campbell R. Harvey and Yan Liu},
   doi = {10.2139/ssrn.3073799},
   journal = {SSRN Electronic Journal},
   keywords = {Anomalies,Backtesting,Bayesian,Factor Zoo,Factors,False discoveries,Multiple testing,Mutual funds,Odds ratio,Power,Smart beta,Type I,Type II},
   month = {12},
   publisher = {Elsevier BV},
   title = {False (and Missed) Discoveries in Financial Economics},
   year = {2017},
}
@book{Hastie2009,
   abstract = {"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates."--Publisher's website.},
   author = {Trevor author Hastie},
   city = {New York, N.Y.},
   edition = {2nd editio},
   editor = {Robert Tibshirani  author and Jerome Friedman  author},
   keywords = {Bioinformatics,Computational Biology,Computational intelligence,Data Mining,Data mining,EstatiМЃstica,EstatiМЃstica computacional,Forecasting,Inference,InfereМ‚ncia estatiМЃstica,Machine learning,Mathematical Computing,MineracМ§aМѓo de dados,Statistics -- Methodology,Statistics as Topic,Supervised learning (Machine learning)},
   publisher = {New York, N.Y. : Springer},
   title = {The elements of statistical learning : data mining, inference and prediction},
   year = {2009},
}
@article{Sharpe1964,
   author = {William F. Sharpe},
   doi = {10.1111/j.1540-6261.1964.tb02865.x},
   issn = {00221082},
   issue = {3},
   journal = {The Journal of Finance},
   month = {9},
   pages = {425-442},
   publisher = {John Wiley & Sons, Ltd},
   title = {CAPITAL ASSET PRICES: A THEORY OF MARKET EQUILIBRIUM UNDER CONDITIONS OF RISK},
   volume = {19},
   url = {http://doi.wiley.com/10.1111/j.1540-6261.1964.tb02865.x},
   year = {1964},
}
@article{Black1972,
   author = {Fischer Black},
   issn = {00219398, 15375374},
   issue = {3},
   journal = {The Journal of Business},
   pages = {444-455},
   publisher = {University of Chicago Press},
   title = {Capital Market Equilibrium with Restricted Borrowing},
   volume = {45},
   url = {www.jstor.org/stable/2351499},
   year = {1972},
}
@article{Fama1992,
   abstract = {Two easily measured variables, size and book‐to‐market equity, combine to capture the cross‐sectional variation in average stock returns associated with market β, size, leverage, book‐to‐market equity, and earnings‐price ratios. Moreover, when the tests allow for variation in β that is unrelated to size, the relation between market β and average return is flat, even when β is the only explanatory variable. 1992 The American Finance Association},
   author = {Eugene F. Fama and Kenneth R. French},
   doi = {10.1111/j.1540-6261.1992.tb04398.x},
   issn = {00221082},
   issue = {2},
   journal = {The Journal of Finance},
   month = {6},
   pages = {427-465},
   publisher = {John Wiley & Sons, Ltd},
   title = {The Cross-Section of Expected Stock Returns},
   volume = {47},
   url = {http://doi.wiley.com/10.1111/j.1540-6261.1992.tb04398.x},
   year = {1992},
}
@article{Lintner1965,
   author = {John Lintner},
   doi = {10.2307/1924119},
   issn = {00346535},
   issue = {1},
   journal = {The Review of Economics and Statistics},
   keywords = {Business -- Business administration -- Business management,Business -- Business operations -- Economic utility,Business -- Business structures -- Commerce,Economics -- Economic disciplines -- Financial economics,Economics -- Microeconomics -- Economic utility},
   pages = {13-37},
   title = {The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets},
   volume = {47},
   year = {1965},
}
@article{Harvey2017,
   abstract = {Indirect incentives exist in the money management industry when good current performance increases future inflows of new capital, leading to higher future fees. We quantify the magnitude of indirect performance incentives for hedge fund managers. Flows respond quickly and strongly to performance; lagged performance has a monotonically decreasing impact on flows as lags increase up to two years. Conservative estimates indicate that indirect incentives for the average fund are four times as large as direct incentives from incentive fees and returns to managers’ own investment in the fund. For new funds, indirect incentives are seven times as large as direct incentives. Combining direct and indirect incentives, for each dollar generated for their investors in a given year, managers receive close to 75 cents in direct performance fees plus the present value of future fees over the expected life of the fund. Older and capacity constrained funds have considerably weaker relations between future flows and performance, leading to weaker indirect incentives. There is no evidence that direct contractual incentives are stronger when market-based indirect incentives are weaker. Contact},
   author = {Campbell R. Harvey},
   doi = {10.2139/ssrn.2893930},
   journal = {SSRN Electronic Journal},
   keywords = {Bayesian P-values,Data dredging,Data mining,MBF,Minimum Bayes Factor,Multiple testing,P-hacking,P-values,Rare incidence,SD-MBF,Selection,Type I error,Type II error},
   month = {1},
   publisher = {Elsevier BV},
   title = {The Scientific Outlook in Financial Economics},
   year = {2017},
}
@article{Kleibergen2009,
   abstract = {We show that statistical inference on the risk premia in linear factor models that is based on the Fama-MacBeth (FM) and generalized least squares (GLS) two-pass risk premia estimators is misleading when the β's are small and/or the number of assets is large. We propose novel statistics, that are based on the maximum likelihood estimator of Gibbons [Gibbons, M., 1982. Multivariate tests of financial models: A new approach. Journal of Financial Economics 10, 3-27], which remain trustworthy in these cases. The inadequacy of the FM and GLS two-pass t/Wald statistics is highlighted in a power and size comparison using quarterly portfolio returns from Lettau and Ludvigson [Lettau, M., Ludvigson, S., 2001. Resurrecting the (C)CAPM: A cross-sectional test when risk premia are time-varying. Journal of Political Economy 109, 1238-1287]. The power and size comparison shows that the FM and GLS two-pass t/Wald statistics can be severely size distorted. The 95% confidence sets for the risk premia in the above-cited work that result from the novel statistics differ substantially from those that result from the FM and GLS two-pass t-statistics. They show support for the human capital asset pricing model although the 95% confidence set for the risk premia on labor income growth is unbounded. The 95% confidence sets show no support for the (scaled) consumption asset pricing model, since the 95% confidence set of the risk premia on the scaled consumption growth consists of the whole real line, but do not reject it either. © 2009 Elsevier B.V. All rights reserved.},
   author = {Frank Kleibergen},
   doi = {10.1016/j.jeconom.2009.01.013},
   issn = {03044076},
   issue = {2},
   journal = {Journal of Econometrics},
   keywords = {Consumption capital asset pricing model,Size distortion of test statistics,Small β's},
   month = {4},
   pages = {149-173},
   publisher = {North-Holland},
   title = {Tests of risk premia in linear factor models},
   volume = {149},
   year = {2009},
}
@article{Gospodinov2017,
   abstract = {This note studies some seemingly anomalous results that arise in possibly misspec- ified, reduced-rank linear asset-pricing models estimated by the continuously updated generalized method of moments. When a spurious factor (that is, a factor that is un- correlated with the returns on the test assets) is present, the test for correct model specification has asymptotic power that is equal to the nominal size. In other words, applied researchers will erroneously conclude that the model is correctly specified even when the degree of misspecification is arbitrarily large. The rejection probability of the test for overidentifying restrictions typically decreases further in underidentified mod- els where the dimension of the null space is larger than 1.},
   author = {Nikolay Gospodinov and Raymond Kan and Cesare Robotti},
   doi = {10.3982/ecta13750},
   issn = {0012-9682},
   issue = {5},
   journal = {Econometrica},
   keywords = {Asset pricing,continuously updated GMM,model misspecification,rank test,reduced‐rank models,spurious risk factors,test for overidentifying restrictions},
   month = {9},
   pages = {1613-1628},
   publisher = {The Econometric Society},
   title = {Spurious Inference in Reduced-Rank Asset-Pricing Models},
   volume = {85},
   year = {2017},
}
@article{Burnside2016,
   author = {Craig Burnside},
   doi = {10.1093/jjfinec/nbv018},
   issn = {1479-8409},
   issue = {2},
   journal = {Journal of Financial Econometrics},
   month = {3},
   pages = {295-330},
   publisher = {Oxford Academic},
   title = {Identification and Inference in Linear Stochastic Discount Factor Models with Excess Returns},
   volume = {14},
   year = {2016},
}
@article{Kan1999,
   abstract = {In this paper we investigate the properties of the standard two-pass methodology of testing beta pricing models with misspecified factors. In a setting where a factor is useless, defined as being independent of all the asset returns, we provide theoretical results and simulation evidence that the second-pass cross-sectional regression tends to find the beta risk of the useless factor priced more often than it should. More surprisingly, this misspecification bias exacerbates when the number of time series observations increases. Possible ways of detecting useless factors are also examined.},
   author = {Raymond Kan and Chu Zhang},
   doi = {10.1111/0022-1082.00102},
   issn = {00221082},
   issue = {1},
   journal = {The Journal of Finance},
   month = {2},
   pages = {203-235},
   publisher = {Blackwell Publishing Inc.},
   title = {Two-Pass Tests of Asset Pricing Models with Useless Factors},
   volume = {54},
   url = {http://doi.wiley.com/10.1111/0022-1082.00102},
   year = {1999},
}
@article{Kleibergen2015,
   abstract = {We construct the large sample distributions of the OLS and GLS R2's of the second pass regression of the Fama and MacBeth (1973) two pass procedure when the observed proxy factors are minorly correlated with the true unobserved factors. This implies an unexplained factor structure in the first pass residuals and, consequently, a large estimation error in the estimated beta's which is spanned by the beta's of the unexplained true factors. The average portfolio returns and the estimation error of the estimated beta's are then both linear in the beta's of the unobserved true factors which leads to possibly large values of the OLS R2 of the second pass regression. These large values of the OLS R2 are not indicative of the strength of the relationship. Our results question many empirical findings that concern the relationship between expected portfolio returns and (macro-) economic factors.},
   author = {Frank Kleibergen and Zhaoguo Zhan},
   doi = {10.1016/j.jeconom.2014.11.006},
   issn = {18726895},
   issue = {1},
   journal = {Journal of Econometrics},
   keywords = {(Non-standard) Large sample distribution,Factor pricing,Fama-MacBeth two pass procedure,Principal components,Stochastic discount factors,Weak identification},
   month = {11},
   pages = {101-116},
   publisher = {Elsevier Ltd},
   title = {Unexplained factors and their effects on second pass R-squared's},
   volume = {189},
   year = {2015},
}
