\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{mathptmx}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage[margin=2.5cm]{geometry}
\usepackage[title]{appendix}
\usepackage{bm}
\usepackage{tcolorbox}
\usepackage{apacite}
\onehalfspacing
\usepackage{lineno}
\linenumbers
\usepackage{diagbox}

\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{fontspec}
\setmainfont{Times New Roman}
\addtolength{\jot}{0.5em}
\linespread{1.5}

\begin{document}
\input{titlepage}
\newpage	
\tableofcontents
\newpage
	\section{Introduction and Motivation}
Capital Asset Pricing Model (CAPM) (\citeA{Sharpe1964}, \citeA{Lintner1965}, and \citeA{Black1972}) introduces a risk pricing paradigm.
The model divided asset's risk into two parts: systematic risk and asset specified idiosyncratic risk.
Researches (see \citeA{Fama1992}, \citeA{Carhart1997}, \citeA{Kelly2019}) has shown that, by adding new factors into the CAPM model, the multi-factor CAPM outperform the initial model in risk pricing.
Because of this, factor identification becomes an important topic in financial.
%Some famous examples of multi-factor CAPM include the Fama-French (FF) three-factor model \cite{Fama1992}, and the Momentum model \cite{Carhart1997}.
% Researchers after them are trying to find new risk factors to include in the multi-factor CAPM model.
\citeA{Harvey2019} had collected over 500 factors from papers published in the top financial and economic journals, and they found the growth of new factors speed up since 2008. 
%In his 2011 presidential address, \citeauthor{Cochrane2011} coined the term "factor zoo" to describe the situation factor modelling is facing: researchers and practitioners have too many options when pricing the risk

%In those famous multi-factor model like FF three-factor model \cite{Fama1992}, the loadings of risk factors pass the significant test comfortably, but this is not always the case. 
But we should notice that not all factors can pass the significant test comfortably almost every time like factors in three-factor model \cite{Fama1992}.
\citeA{Pesaran2019} provide a criteria called factor strength to measure such discrepancy.
In general, if a factor can generate loading significantly different from zero for all assets, then we call such a factor a strong factor.
And the less significant loading a factor can generate, the weaker the strength it has.

In his 2011 president address \citeauthor{Cochrane2011} emphasis the importance of finding factor which can provides independent information about average return and risk.
With regard of this, a number of scholars had applied various methods to find such factor. 
%Since in their 2015 research \citeA{Harvey2015} argues that the current threshold for a test of parameter significant is too low for newly proposed factors, 
\citeA{Harvey2017} provided a bootstrap methods to adjust the threshold of factor loading's significant test, trying to exclude some falsely significant factor caused by multiple-test problem.
Some other scholars use machine learning methods to reduce the potential candidates, more precisely, a stream of them have used a shrinkage and subset selection method called Lasso \cite{Tibshirani1996} and it's variations to find suitable factors.
One example is \citeA{Rapach2013}.
They applied the Lasso regression, trying to find some characteristics from a large group to predict the global stock market's return.

But an additional challenge is that factors, especially in the high-dimension, are commonly correlated.
%\citeA{Cochrane2005} argues that the correlation between factors will reduce the ability of using risk premium to infer factors.
\citeA{Kozak2020} point out that when facing a group of correlated factors, Lasso will only pick several highly correlated factors seemly randomly, and then ignore the other and shrink them to zero. 
In other word, Lasso fails to handle the correlated factor appropriately.


%\citeA{Kan1999} found that the test-statistic of FM two-stage regression  \cite{Fama1973} will inflate when incorporating factors which are independent with the cross-section return.
%Provides spurious factors with the chance  to pass the significant test.




%\citeA{Hou2018} found that they can not replicate the result of over 60\% selected factor papers. 

%These discoveries cast doubt on the risk pricing ability of some factors, indicates that some factors may fail to pass the significant test in certain scenarios.


Therefore, the main empirical problem in this project is: how to select useful factors from a large group of  highly correlated candidates.
To answer this problem, we employed a two-step method. 
First, we consider the selection of factor base on their strength.
And then we will use another variable selection method called Elastic Net \cite{Zou2005} to select factors.
With regard of the first stage, \citeA{Bailey2020} provides a consistent estimates method for the factor strength, and we will use such method to exam the strength of each candidate factors, and filter out those spurious factors, therefore,reduce the dimension of the number of potential factors.
For the second part, elastic net fixes the problem of Lasso can not handle correlated variables by adding extra penalty term, which makes it suitable for our purpose. 

For the rest of this plan, we will first go through some literatures relates with CAPM model and methods about selecting factors. 
Then we will give detailed description of factor strength and elastic net, as well as some preliminary  discoveries about factor strength.
Finally, we will talks about the further plan for this project.




%For this project, we are trying to deal with the classical problem: what factors can have significant contributions to explain the asset risk and return relationship under the CAPM framework.
%And we go a step forward, take the correlation among all factors into the account. 
%ut before applying the Elastic Net method, we want to first investigate all those factor's strength.
%Our interest is focused on first determine which factors have enough power to help us solve the risk pricing problem. 
%Then, from this pre-determined relatively small factor group, we applied the Elastic Net method, trying to find out the most appropriate factors to help us form the multi-factor CAPM model.














	\section{Related Literature}
%This project combines three  kinds of literatures: CAPM, factor strength, and factor selection under high dimensioned setting for the number of potential factors.

This project is builds on papers devoted on  risk pricing.
Formularised d by \citeA{Sharpe1964}, \citeA{Lintner1965}, and \citeA{Black1972}, the CAPM model only contains the market factor, which is denotes by the difference between market return and risk free return.
\citeA{Fama1992} develop the model into three-factors, and then it been extend it into four \cite{Carhart1997}, and five \cite{Fama2015}.
Recent research created a six-factors model and claim it outperform all other sparse factor model. \cite{Kelly2019}.

This project also connect with papers about involving factors has no or weak correlation with assets' return into CAPM model.
\citeA{Kan1999} found that the test-statistic of FM two-stage regression  \cite{Fama1973} will inflate when incorporating factors which are independent with the cross-section return.
%Provides spurious factors with the chance  to pass the significant test.
\citeA{Kleibergen2009} pointed out how a factor with small loading would deliver a spurious FM two-pass risk premia estimation. 
%\citeA{Kleibergen2015} found out even if some factor-return relationship does not exist, the r-square and the t-statistic of FM regression would in favour of the conclusion of such structure presence. 
\citeA{Gospodinov2017} show how the involving of a spurious factor will distort the statistical inference of parameters.
And, \citeA{Anatolyev2018} studied the behaviours of the model with the presence of weak factors under asymptotic settings, find the regression will lead to an inconsistent risk premia result.
	
	
This project also relates to some researches effort to identify useful factors from a group of potential factors.
\citeA{Harvey2015} exam over 300 factors published on journals, presents that the traditional threshold for a significant test is too low for newly proposed factor, and they suggest to adjust the p-value threshold to around 3. 
Methods like a Bayesian procedure introduced by \citeA{Barillas2018} were used to compare different factor models.
\citeA{Pukthuanthong2019} defined several criteria for "genuine risk factor", and base on those criteria introduced a protocol to exam does a factor associated with the risk premium.


This project will attempt to address the factor selection problem by using machine learning techniques.
\citeA{Gu2020} elaborate the advantages of using emerging machine learning algorithms in asset pricing such as more accurate predict result, and superior efficiency.
Various machine learning algorithms have been adopted on selecting factors for the factor model, especially in recent years.
\citeA{Lettau2020} applying Principle Components Analysis on investigating the latent factor of model. 
Lasso method, since it's ability to select features, is popular in the field of the factor selection.
\citeA{Feng2019} used the double-selected Lasso method \cite{Belloni2014},and a grouped lasso method \cite{Huang2010} is used by \citeA{Freyberger2020} on picking factors from a group of candidates. 
\citeA{Kozak2020} used a Bayesian-based method, combing with both Ridge and Lasso regression, argues that the sparse factor model is ultimately futile. 


	\section{Methodology}
			\subsection{Factor Strength}\label{strength}
Capital Asset Pricing Model (CAPM) is the benchmark for pricing the systematic risk of a portfolio. 
Consider the following multi-factor models for n different assets and T observations with stochastic error term $\varepsilon_{it}$:
\[   r_{it} - r_{ft} = a_i + \beta_{im}(r_{mt} - r_{ft}) + \sum_{j=1}^{k}\beta_{ij}f_{jt} + \varepsilon_{it} \tag{1}\label{2CAPM} \]
In the left-hand side, we have $r_{it}$ denotes the return of  security i at time t, where $i = 1, 2,3, \cdots, N$ and $t = 1,2,3, \cdots T$.  
$r_{ft}$ denotes the risk free rate at time t.
In the other hand, $a_i$ is the constant term. 
$r_{mt}$ is the market average return and therefore, $(r_{mt} - r_{ft}) $ is the excess return of the market. 
Corresponding $\beta_{im}$ is the lading of market excess return or market factor.
$f_{jt}$ of $j = 1, 2, 3\cdots k$ is potential risk factor under consideration.
$b_{ij}$ represents the factor loading for each k risk factors.

The factor strength of factor $f_{jt}$ as $\alpha_j$ from \citeA{Pesaran2019}, and \citeA{Bailey2020} is defined as the pervasiveness of a factor.

If we run the OLS regression for equation (\ref{2CAPM}) with only one facto  $f_{jt}$, we will obtain n different factor loading $\hat{\beta}_{it}$. For each of the  factor loading $\hat{\beta}_{ij}$, we can construct a t-test to test does the loading equals to zero. The test statistic will be $t_{jt} = \frac{\hat{\beta}_{ij} - 0}{\hat{\sigma}_{jt}}$ where $\hat{\sigma}_{i}$ is the standard error of $\hat{\beta}_{ij}$.  
Then we defined $\pi_{nT}$ as the proportion of significant factor's amount to the total factor loadings amount:

\[  \hat{\pi}_{nT} = \frac{\sum_{i=1}^n \hat{\ell}_{i,nT}}{n} \tag{2} \label{pi_function} \]

$\ell_{i,nT}$ is an indicator function as: $\ell_{i,nT} := {\bf1}[|t_{jt}|>c(n)]$. 
If the t-statistic $t_{jt}$ is greater than the critical value $c_p(n)$,  $\hat{\ell}_{i,nT} = 1$. 
In other word, we will count one if the factor loading $\hat{\beta}_{ij}$ is significant. 
$c_p(n)$ represent the critical value of a test with test size $p$. 
The critical value is calculated by:

\[   c_p(n) = \Phi^{-1}(1 - \frac{p}{2n^\delta})   \tag{3} \label{critical_value_function} \]

Here, $\Phi^{-1}(\cdot)$ is the inverse cumulative distribution function of a standard normal distribution, and $\delta$ is a non-negative value represent the critical value exponent. 
The traditional method to calculate critical value has not fixed the multiple testing problem. 
One of the most commonly used adjustment for multiple testing problem is Bonferroni correction. 
When $n$ as sample size goes to infinity, however, the Bonferroni correction can not yield satisfying results since the $\frac{p}{2n^{\delta}} \to 0$ when $n \to \infty$. 
Therefore, \citeA{Bailey2016} provides another adjustment with additional exponent $\delta$ to constrain the behaviour of $n$.

After obtain the $\hat{\pi}_{nT}$, we can use the following formula to estimate our strength indicator $\alpha_j$:
\[ \hat{\alpha} = \begin{cases}
1+\frac{\ln(\hat{\pi}_{nT})}{\ln n} & \text{if}\; \hat{\pi}_{nT} > 0,\\
0, & \text{if}\; \hat{\pi}_{nT} = 0.
\end{cases} \]
From the estimation, we can find out that $\hat{\alpha} \in [0,1]$

$\hat{\alpha}$ represent the pervasiveness of a factor. 
Here we denote $[n^{\alpha}]$ , $[\cdot]$ will take the integer part of number inside. 
For factor $f_{jt}$:

\begin{align*}
|f_{jt}| &> c_p(n)\quad i = 1, 2,  \dots, [n^{\alpha_j}]\\
|f_{jt}| &= 0 \quad i = [n^{\alpha_j}] + 1, [n^{\alpha_j}] +2 ,\dots, n
\end{align*}
For a factor has strength $\alpha = 1$,  factor loading will be significant for every assets at every time. 
The more observation the factor can significantly influence, the stronger the factor is, and vice versa.
Therefore, we can use the factor strength to exclude those factor has only very limited pricing power, in other word, those factor can only generate significant loading on very small portion of assets. 

\subsection{Elastic Net}

Elastic net  is variable selection model that can be used for factor selection, introduced by \citeA{Zou2005}. Applying elastic net method to estimate the factor loading $\beta_{ij}$ requires:
	\[   \hat{\beta}_{ij}  = \argmin_{\beta_{ij}}\{\sum_{i = 1}^{n}[(r_{it} - r_{ft}) - \beta_{ij }f_{jt}]^2 + \lambda_2\sum_{i = 1}^{n}\beta_{ij}^2  + \lambda_1\sum_{i = 1}^{n}|\beta_{ij}|  \label{ENcriterion} \tag{4}   \}    \]
	
	
	
Because the Lasso regression only contains $L_1$ penalty term $\sum_{i = 1}^{n} |\beta_{ij}|$, it will shows no preference when selecting variables when they are highly correlated.
So when Lasso regression will either randomly choose factors from highly correlated candidates, or eliminate them together as a whole.  
Elastic Net, however, by containing $L_2$ penalty term $\sum_{i=1}^{n}\beta_{ij}^2$, solves this problem. 
The $L_2$ penalty term tend to shrink the potential parameters when they does not provide enough explanatory power, but it will not remove redundant factors.
Therefore, the elastic net method will shrink those parameters associated with the correlated factors and keep them, or drop them if they are redundant at pricing risk. 
	


\section{Preliminary Result}\label{preli}
In current stage, we have studied the property of estimator of factor strength $\alpha$ under finite sample scenario.
In purpose of this, we have designed and applied a Monte Carlo Simulation.
The design details and result table can be find at the Appendix \ref{MC} and Appendix \ref{simulationtable} 

To measure the goodness of simulation, we calculate the difference between the estimated factor strength and assigned true factor strength and refer the difference as bias.
Base on the bias, we also calculated the Mean Squared Error (MSE) for each setting.

The table shows that both the bias and MSE of $\hat{\alpha}$ for different value of the $\alpha$, N and T.

From the table \ref{simutable1} and table \ref{simutable2} we can see that when the $\alpha$ is at a relatively low level, the estimator  tends to overestimate the strength.
For instance, under the  setting of $T = 120, N = 500$ and $\alpha = 0.5$, the bias is over 0,2, indicates that the estimated strength $\hat{\alpha}$ is around 0.7.
The overestimation, however decreased with the increase of the $\alpha$.
Under the same setting as above, if the strength is assigned to 0.9, the bias significantly reduced to only 0.023.
When the $\alpha$ touched it's upper bound as $\alpha = 1$, the bias disappears.
Therefore we can conclude that the error converge to zero when the strength $\alpha$ increases. 
Precision of this estimator improves as $\alpha$ increases toward unity for given T and N increases as well.

			\section{Further Plan}
For the next step of this project, we will start the empirical analyse. 
Considering the variations of companies included, we will use companies return from Standard \& Poors (S\&P) 500 index as assets, and examine the strength of potential factors (150 in total) to be included in the final factor model specification. 
The time span will be 10 years, and in order to cover both recession and flourish, we use data from 2008  to 2018.
The return will be calculated on monthly basis, so the time observation for each stock will be t = 120. 



\newpage
\bibliographystyle{apacite}
\bibliography{proposal.bib}

\newpage
\appendix

\input{appendix.tex}
\end{document}