\chapter{Empirical Application: Elastic Net}\label{Empirical:Elastic_net}
As we introduced before, the surging number of risk factors post question of which factors can independently provides unique information to explain the risk-return relationship. \cite{Cochrane2011}.
In this chapter, we employed the Elastic net algorithm to discuss this problem.
The application is build on the basis of factor strength we estimated in the previous chapter.
Factor strength provides a criterion to reduce the dimensions of potential factor groups.
The factor strength also works as a reference when evaluating the performances of algorithms.
We would expect the method tend to select factor with strong strength than weak one.
For the rest of this chapter, we first briefly introduce the core idea of the Elastic net method, explained what's different between Elastic net and other factor selection methods, especially the Ridge regression and Lasso regression.
Then, we provide a full discussion of how to select the tuning parameter with regard to the Elastic net method when using r package \textit{glmnet} to imply the Elastic net.
Finally, we compare and discuss the differences selection results of both elastic net methods and Lasso.


\section{Brief Introduction to Elastic Net} \label{Elastic_Net}

Elastic net,introduced by \citeA{Zou2005},  is a penalised linear regression method which developed on the basis of the Lasso regression \cite{Tibshirani1996} and Ridge regression.
To illustrate the application of the Elastic net method in our research, recall the multi-factor model (\ref{multi_factor_model}) we discussed in chapter \ref{strength_multi_estimation}.
When applying the OLS to estimate the factor loading $\bm{\beta_{i}}$ of model (\ref{multi_factor_model}), we targeting minimise the Residual Sum of Squares (RSS):
\[  \bm{\hat{\beta}}_i =   \argmin\{  (x_{it} - \hat{a}_{iT} - \bm{\hat{\beta}}_i^{\prime}\bm{f}_t )^2 \}    \]
The OLS method will consider all factors proposed by users when constructing the multi-factor CAPM model.
This means that even factor with weak strength, which can not bring any new information to price the risk of assets will generate loadings under the OLS method.
The Elastic net method, although also focusing on minimising RSS, including two extra penalty terms inside the loss function.
\[   \bm{\hat{\beta}_{i}}  = \argmin_{\beta_{ij}}\{ (x_{it} - \hat{a}_{iT} - \bm{\hat{\beta}}_{i} ^{\prime}\bm{f}_{t})^2 + \lambda_2\sum_{j = 1}^k\hat{\beta}_{ij}^2  + \lambda_1\sum_{j=1}^k|\hat{\beta}_{ij}|  \label{ENcriterion} \tag{11}   \}    \]
Here, the estimated $\hat{\bm{\beta}}_i$ loading value is subject to two penalty terms: the $L^1$ norm $\lambda_1\sum_{j=1}^k|\hat{\beta}_{ij}|$ and the $L^2$ norm: $\lambda_2\sum_{j = 1}^k\hat{\beta}_{ij}^2$.
The Elastic net estimation, in essence, is a combination method of Ridge regression and the Lasso regression.
If setting $\lambda_1 = 0$, the Elastic net will collapse into the Lasso regression, and if $\lambda_2 = 0$, we will obtain result of the Ridge regression.
In the empirical application of this paper, the Elastic net estimation uses the following form \cite{Friedman2010}:
\[		\bm{\hat{\beta}}_{i} = \argmin_{\beta_{ij}}\{ \frac{1}{2N} (x_{it}-\hat{a}_{iT} - \bm{\hat{\beta}_{i}^{\prime}}\bm{f}_t ^2 ) +\phi P_{\theta}(\bm{\beta}_i)  \} \label{EN:empirical_formula}\tag{12} \]
\[	P_{\theta}(\bm{\beta}_i) =\sum_{j=1}^k [ (1-\theta)\beta_{ij}^2 + \theta |\beta_{ij}|] \label{EN:elastic_net_penalty} \tag{13}\]
Here we call the $P_{\theta}(\cdot)$ as the elastic net penalty.
$\theta$  acts as the turning parameter to determine how will the elastic net penalty is combined by the $L^1$ and $L^2$ norms.
When set $\theta = 1$, we have $P_{\theta}(\bm{\beta}_i) =\sum_{j=1}^k  |\beta_{ij}|$ which is identical to the $L^2$ norm, therefore we have the Elastic net collapse to the Lasso regression.
Similarly, when setting $\theta = 0$, we have the Elastic net collapse to the ridge regression, and when $\theta = 0.5$, the Elastic net is behave like the combination of ridge and Lasso.
The other tuning parameter $\phi$ decides how strong the penalty terms is.
If $\phi = 0$ the elastic net will become the OLS estimation.

In this study, we use the data introduced in chapter \ref{data}, and the estimated factor strength from the chapter \ref{Empirical:factor_strength}.
We will briefly review risk factors with their strength in the next chapter.
More specifically, we allocates the 145 risk factors into six subgroups base on there thirty-year estimated strength.
For each subgroup, we want to investigate how will the Elastic net algorithm, alongside the Lasso regression, select the risk factors for company stock.
We would expect that when facing factor with strong strength, the algorithm will construct a dense factor model, and with the factor strength decrease, the density will decrease simultaneously.
Notice that in order to simplify the application of the Elastic net, instead of using the excess return of stock directly, we first run an OLS regression between the market factor and the excess return of each company.
And then, we collect the residuals of the OLS estimation, use the residuals as the left hand side term of the multi-factor CAPM model.
This step helps us to remove the potential influence of market factors, allowing the algorithms only focussing on the risk factors. 

Now, the main challenge of applying the elastic net algorithm is to select the appropriate tuning parameters $\theta$ and $\phi$, and we will discuss the choices of tuning parameter in the following section.

\section{Properties of Risk Factors}
In this chapter, we briefly review the properties of the risk factors series. 
The risk factors data set contains 145 risk factors at the monthly frequency for the period from July 1976 to December 2007.\footnote{For how those factors are constructed, please view \citeA{Feng2020}}
As a standard practice of time series data, we exam the stationarity of the risk factor series by employing Augmented Dick-fuller (ADF) test \cite{Dickey1979},  Phillips-Perron (PP) test \cite{Phillips1988}, and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test \cite{Kwiatkowski1992}.
Overall, the ADF test and PP test provides the same conclusions that all 145 risk factor series does not contain unit root.
The KPSS test, however, disagrees with the ADF and PP test.
If we take 0.05 as the threshold of p-value, the KPSS test concludes that there are six factors that contain unit root in their process.
We also present the correlation heat map of all 145 risk factors in the Figure \ref{figure:correlation} at the appendix \ref{factor_correlation}.
All risk factors are sorted base on their estimated factor strength.
The dark lower-left area of the Figure \ref{figure:correlation} indicates that factor with strong estimated strength presents a high correlation with other strong factors.
With the factor strength decrease, the correlation coefficient among factors decreases significantly.
If focusing on the upper-right corner of the figure, where weak factors are clustering, we can see that the correlation coefficients are close to zero.
By observing the upper-left corner and the lower-right corer, we can observe that the correlation between weak factors and strong factors are also very low


\section{Tuning Parameter} \label{EN:parameter_tuning }
In this empirical application, we use the R package \textit{glmnet} \cite{Friedman2010, Simon2011}.

From the equation (\ref{EN:empirical_formula}) and (\ref{EN:elastic_net_penalty}) we know that the estimation of Elastic net dependent on two tuning parameters $\theta$ and $\phi$.
The \textit{glmnet} package provides function to select the $\phi$ automatically. 
This selection is base on the minimisation of Mean Squared Error (MSE), using cross-validation.
However, the package does not provide aid on determining which value of $\theta$ parameter is optimal.
Therefore, we adopt the following strategy to select our tuning parameters $\phi$  and $\theta$:
\begin{enumerate}
\item Prepare a sequence of $\theta$ values, from 0: ridge regression, to 1: Lasso regression with the step of 0.01
\item Randomly assign 90\% of the data set as the training set and the rest 10\% as the test set. 
\item For each of the $\theta$ value, we fit the corresponding Elastic net model using the training set, with function picked $\phi$ values.
\item Base on the $\theta$ and $\phi$ values select, we produce the predicted values using the test data, and calculate the MSE between the true values and predicted values.
\item We select the $\theta - \phi$ combination which minimises the MSE.
\end{enumerate}
We repeat the above procedures 2000 times for each factor strength group and
calculates the $\bar{\theta}$ by averaging every $\theta$ we obtained from the repetition.
% average each $\theta$ values to generate $\bar{\theta}$, as our selected tuning parameter.
Due to the computational burden, when implying step 2, instead of using the full sample of stocks and factors, we randomly select 50 companies from 242 all companies, and 10 factors from each subgroups.
The selected tuning parameter $\theta$ values are shown in table \ref{table:optimal_theta}.
\begin{table}[H]
	\centering
	\caption{Estimated Optimal $\theta$ values for different factor groups}
	\label{table:optimal_theta}
	\begin{tabular}{l|cccc}
		\hline
		\hline
		Factor Group            & (0, 0.5{]}   & (0.5, 0.6{]} & (0.6, 0.7{]} & (0.7, 0.8{]} \\ 
		Selected $\theta$ value & 0.377        & 0.401        & 0.429        & 0.411        \\ \hline
		Factor Group            & (0.8, 0.9{]} & (0.9, 1{]}   & Mix          & Random       \\ 
		Selected $\theta$ value & 0.396        & 0.413        & 0.448        & 0.431        \\ \hline
		\hline
	\end{tabular}
\end{table}
In order to fully investigate the behaviours of parameter tuning, on the basis of the six subgroups, we create tow addition groups.
The Mix group contain five highly strong factors: factor with strength higher than 0.9, and five weak factors: factor with strength lower than 0.5.
The Random group consist of ten randomly selected factors from all 145 risk factors.
From table \ref{table:optimal_theta} we can see that the selected $\theta$ value in general increase with the factor strength increase.
For weak factor group, the selected parameter value is 0.377, close to the ridge regression.
While for the strong factor group, the value is 0.413.
The mix factor group has the highest $\theta$ value of 0.448.

Such a pattern of the $\theta$ value, however, does not follow what we expected.
The definition of factor strength indicates that factor with strong strength is able to produce more significant loadings, in other words, can explain more assets' risk-return relationship.
Therefore, when using the above procedure to decide tuning parameter, we would expect that for factor groups with lower strength, like groups with strength smaller than 0.5, the selected $\theta$ parameter will be close to one, or larger than other groups' selected $\theta$.
This is because factor with weak strength will only provide limited pricing power, and therefore may be recognised by the algorithm as redundant variables.
When $\theta$ is closer to unity, the elastic net is behaved more like a Lasso, which will eliminate variables provides limited explaining power.
In contrast, when the group has stronger strength, the $\theta$ will approach closer to zero, leads the Elastic nets more like Ridge, which will not eliminate any variables, but only reduce the coefficient.
So we would expect the $\theta$ value to increase with the group strength decrease.

We prepared several potential explanations for this result.
First, the MSE is not an ideal criterion for selecting the tuning parameter.
The MSE for all $\theta - \phi$ combinations are very close to each other.
All MSE results show similar values around 64.
Second, because of the estimation method we used, the market risk has already been absorbed by the market factors.
Then for the strong factors, we would expect that for any single of them, those strong factor can individually explain most of the idiosyncratic risk.
Therefore, when we ask any ten strong factors to determine the risk simultaneously, it is possible that very few of the ten factors can explain most of the risk and therefore the other risk factors will be recognised by the algorithm as redundant.
But if all factors are weak, it is possible that there exist some linear combinations among weak factors provides enough explaining power for the idiosyncratic risk.
Therefore, those weak factors will be reserved by the algorithm, and hence, the parameter $\theta$ will close to zero, indicates a more Ridge like regression.





\section{Elastic Net Findings}
We applied the elastic net algorithm with tuning parameters estimated in the previous chapter using the thirty-year data set.
The reasons why we use the thirty-year data instead of ten or twenty years is because the Monte Carlo simulation result (see chapter \ref{MC_findings} and Appendix \ref{simulationtable}) indicates us that the estimation accuracy of factor strength will improve significantly with a decreasing of size of the test, as the increasing of time spam.
Even for the data set with limited observations, estimation shows high accuracy when t is large.
Therefore, we use the factor strength estimated from the thirty-year data set.

We divided the 145 risk factors into six groups base on their factor strength.
We also randomly selected ten factors from  weak factor group (factor with strength less than 0.5), and ten factors from strong factor group (factor with strength above 0.9) to form a mixed factor group.
For each factor groups, we run two regression: the elastic net regression with $\theta$ values presented in table \ref{table:optimal_theta}, and the Lasso regression with $\theta = 1$.
Instead of running a pooled regression, we run the elastic net and Lasso for each individual company, and record the result of factor selection of every stock.
First, we focusing on the general behaviours of factor selection between our two methods.
Table \ref{table:select_prop} presents the average factor selection amount for each factor groups of two selection methods.
\begin{table}[h]
	\centering
		\caption{Average factor selection proportions and factor selection counts of Elastic Net and Lasso}
		\label{table:select_prop}
		\resizebox{\textwidth}{!}{
	\begin{tabular}{l|ccccccc}
		\hline
		\hline
		Factor Group                   & (0,0.5] & (0.5, 0.6]& (0.6, 0.7] & (0.7, 0.8] & (0.8,0.9] & (0.9,1] & Mix \\
		Factor Amount                  & 12            & 10               & 17               & 37               & 35              & 34            & 20  \\ \hline
		Avg EN selection amount        & 2.11          & 4.47             & 8.67             & 14.67            & 13.51           & 12.37         & 8.45                    \\
		Avg EN selection proportion    & 17.5\%        & 44.73\%          & 51.00\%          & 39.65\%          & 38.61\%         & 36.38\%       & 42.28\%                 \\
		Avg Lasso selection amount     & 2.06          & 3.87             & 8.43             & 13               & 12.19           & 10.46         & 7.26                    \\
		Avg Lasso selection proportion & 17.2\%        & 38.76\%          & 49.60\%          & 35.14\%          & 34.83\%         & 30.75\%       & 36.27\%                 \\ \hline\hline
	\end{tabular}
}
\end{table}
We can see that the factor model selected by the Lasso regression, on average, is more parsimonious than the model selected by the Elastic net.
And this trend becomes more and more obvious with the factor strength increase.
For the weak factor group with strength less than 0.5, Elastic net and Lasso provides a similar answer: only two factors are selected out of twelve candidates.
While when focusing factor with strength above 0.9, Lasso will select almost 2 fewer factors than the Elastic net.
Such outcome is not surprising, since the tuning parameter $\theta$ of Elastic net is closer to 0 than 1, which means that the Elastic net algorithm in our application tend to keep the factors even though they can provide very limited information.
Unexpectedly, the proportion of factor selection to the strong-factor group is significantly lower than the semi-strong group.
For the factor group with strength between 0.6 and 0.7, both Elastic net and Lasso select almost half of 17 candidates factors. 
But when facing strong factors with strength above 0.9, Elastic net on average only keep about 36\% factors.


We then compare every single stock's factor selecting decision made by Lasso and Elastic net, and calculates in what degree those two methods will agree with each other.
For every single company, if both Elastic net and Lasso select the same factor (generates factor loading not equals to zero), and disregard the same factor (generate factor loadings equal to zero), we call Elastic net and Lasso made a exact agreement of factor selecting.
We also extend our standard of the agreement to 90\% interval. 
If the Elastic net and Lasso achieve agreement in 90\% of factors' selection results, we would call they made a agreement on 90\%.
The proportion of agreement is presented in the table \ref{table:proportion_agreement}.

\begin{table}[h]
	\centering
	\caption{Proportion of Lasso Regression and Elastic Net produces same or largely similar results for 145 companies}
	\label{table:proportion_agreement}
	\begin{tabular}{c|ccccccc}
		\hline
		\hline
		\multicolumn{1}{l|}{Factor Group}                                         & \multicolumn{1}{l}{(0,0.5{]}} & \multicolumn{1}{l}{(0.5, 0.6{]}} & \multicolumn{1}{l}{(0.6, 0.7{]}} & \multicolumn{1}{l}{(0.7, 0.8{]}} & \multicolumn{1}{l}{(0.8,0.9{]}} & \multicolumn{1}{l}{(0.9,1{]}} & Mix    \\ \hline
		\begin{tabular}[c]{@{}c@{}}Proportion of Agreement\\ (Exact)\end{tabular} & 68.7\%                        & 55.9\%                           & 42.8\%                           & 20.9\%                           & 17.7\%                          & 13.9\%                        & 34.6\% \\
		\begin{tabular}[c]{@{}c@{}}Proportion of Agreement\\ (90\%)\end{tabular}  & 86.8\%                        & 72.0\%                           & 74.5\%                           & 72.0\%                           & 79.8\%                          & 74.4\%                        & 76.1\% \\ \hline\hline
	\end{tabular}
\end{table}

It is clear that the proportion of agreement, both exact agreed and 90\% agreed, shows a decreasing trend with the factor strength increase.
Nearly 70\% of factors selection results are identical in the 0 to 0.5 strength group, but this number will decrease to 55\% after we move to the 0.5-0.6 factor strength group.
Only 14\% of companies have identical factor selection results for group with strength between 0.9 and 1.
For the mixed strength group, the exact agreed proportion is 34.6\%, ranked between the 0.6 to 0.7 group and 0.7 to 0.8 group.

To provide an explanation of the disagreement increasing with factor strength increasing, we calculate the correlation among factors for each factor groups.
The result is presented in table \ref{table:Correlation} and Figure \ref{figure:correlation}.
\begin{table}[h]
	\centering
	\caption{Correlation Coefficient among different factor groups. }
	\label{table:Correlation}
	\begin{tabular}{l|cccccc}
		\hline
		\hline
		Factor Group                                 & (0,0.5{]} & (0.5, 0.6{]} & (0.6, 0.7{]} & (0.7, 0.8{]} & (0.8,0.9{]} & (0.9,1{]} \\ \hline
		\multicolumn{1}{c|}{Correlation Coefficient} & 0.0952    & 0.157        & 0.213        & 0.229        & 0.371       & 0.724   \\
		Factor Amount &12 & 10 &  17 & 37& 35 &34  \\ \hline \hline
	\end{tabular}
\end{table}
We can clearly see an increasing trend of the correlation coefficient with the increase of factor's strength.
This correlation pattern provides a possible explanation of the discord results of Elastic net and Lasso.
When facing variables with high correlation, Lasso will randomly select several variables and discard others.
While Elastic net address this problem with the help of the extra $L^2$ norm in its loss function.
Therefore, the Elastic net method will select all factors that can bring new information to explain the risk-return relationship even those factors are highly correlated.
This also provides a potential explanation to what we observed in the table \ref{table:proportion}  that Lasso selects significantly fewer factors than the Elastic net method.
Also, recall the fact that in the strong factor group, Elastic net will select two more factors than the Lasso on average.
We believe Elastic net here will pick up factors that been abandoned by the Lasso due to the correlativity.
