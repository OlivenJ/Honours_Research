\chapter{Empirical Application: Elastic Net}\label{Empirical:Elastic_net}
As introduced in the previous chapter, the high-dimension risk factor group posts challenge when selecting factors.
We want to identify factor that can independently provide information to explain the risk-return relationship. \cite{Cochrane2011}.
In this chapter, we employ the Elastic net algorithm to discuss this problem.
The application is build on the basis of factor strength we estimated in the chapter \ref{Empirical:factor_strength}.
Factor strength provides a criterion to reduce the dimensions of potential factor group.
The factor strength also works as a reference when evaluating the performances of algorithms.
For the rest of this chapter, we first briefly introduce the core idea of the Elastic net method, explain what's different between Elastic net and other factor selection methods, especially the Ridge regression and Lasso regression.
Then, we provide a full discussion of how to select the tuning parameter with regard to the Elastic net method when using r package \textit{glmnet}.
Finally, we compare and discuss the differences selection results of both Elastic net methods and Lasso.


\section{Brief Introduction to Elastic Net} \label{Elastic_Net}

Elastic net,introduced by \citeA{Zou2005},  is a penalised linear regression method which developed on the basis of the Lasso regression \cite{Tibshirani1996} and Ridge regression.
To illustrate the application of the Elastic net method in our research, recall the multi-factor model (\ref{multi_factor_model}) we discussed in chapter \ref{strength_multi_estimation}.
%In that setting, we have $i = 1,2,\cdots, n$ cross-section unit, $j = 1,2,\cdots, k$ risk factors, and for all unit and factor, each of them have $t = 1,2,\cdots, T$ time-series observation.
When applying the OLS to estimate the factor loading $\bm{\beta_{i}} = \{ \beta_{i1}, \beta_{i2}, \cdots, \beta_{ij}   \}$ of corresponding factor $\bm{f_{t}} = \{ f_{1t}, f_{2t}, \cdots, f_{jt} \}$ for model (\ref{multi_factor_model}), we targeting minimise the residual sum of squares (RSS):
\[  \bm{\beta}_i =   \argmin\{  (x_{it} - a_{iT} - \bm{\beta}_i^{\prime}\bm{f}_t )^2 \}    \]
The OLS method will consider all factors proposed by users when constructing the multi-factor CAPM model.
%This means that even factors with weak strength, which can not bring any new information to price the risk of assets will generate loadings under the OLS method.
This means that even factors with weak or no relationship with the assets, will generate loadings under the OLS method.
%The Elastic net method, although also focusing on minimising RSS, including two extra penalty terms inside the loss function.
The Elastic net method, although also focusing on minimising RSS, avoid this problem by adding two extra penalty terms.
When applying the Elastic net, we focusing on the following loss function:
\[   \bm{\beta}_{i}  = \argmin_{\beta_{ij}}\{ (x_{it} - a_{iT} - \bm{\beta}_{i} ^{\prime}\bm{f}_{t})^2 + \lambda_2\sum_{j = 1}^k\beta_{ij}^2  + \lambda_1\sum_{j=1}^k|\beta_{ij}|  \label{ENcriterion} \tag{11}   \}    \]
Here, the estimated $\bm{\beta}_i$ loading value is subject to two penalty terms: the Lasso penalty $\lambda_1\sum_{j=1}^k|\beta_{ij}|$ and the Ridge penalty $\lambda_2\sum_{j = 1}^k\beta_{ij}^2$.
The Elastic net estimation, in essence, is a combination method of Ridge regression and the Lasso regression.
If setting $\lambda_1 = 0$, the Elastic net will collapse into the Lasso regression, and if $\lambda_2 = 0$, we will obtain same result as the Ridge regression.
In the empirical application of this thesis, the Elastic net estimation uses the following loss function \cite{Friedman2010}:
\[		\bm{\beta}_{i} = \argmin_{\beta_{ij}}\{ \frac{1}{2N} (x_{it}-a_{iT} - \bm{\beta_{i}^{\prime}}\bm{f}_t ^2 ) +\phi P_{\theta}(\bm{\beta}_i)  \} \label{EN:empirical_formula}\tag{12} \]
\[	P_{\theta}(\bm{\beta}_i) =\sum_{j=1}^k [ (1-\theta)\beta_{ij}^2 + \theta |\beta_{ij}|] \label{EN:elastic_net_penalty} \tag{13}\]
Here \citeauthor{Zou2005} call the $P_{\theta}(\bm{\beta}_{i})$ as the Elastic net penalty.
Parameter $\theta$ acts as the turning parameter to determine how will the Elastic net penalty is combined by the Lasso penalty and Ridge penalty.
When set $\theta = 1$, we have $P_{\theta}(\bm{\beta}_i) =\sum_{j=1}^k  |\beta_{ij}|$ which is identical to the Lasso penalty.
Therefore we have the Elastic net collapse to the Lasso regression when $\theta = 1$.
Similarly, when setting $\theta = 0$, we have the Elastic net collapse to the Ridge regression. 
%and when $\theta = 0.5$, the Elastic net is the half-half combination of Lasso and Ridge.
The other tuning parameter $\phi$ decides how strong the penalty terms is.
If $\phi = 0$ the Elastic net will become the OLS estimation.

In this study, we use the data introduced in chapter \ref{data}, and the estimated factor strength from the chapter \ref{Empirical:factor_strength}.
We will briefly review risk factors with their strength in the next section.
More specifically, we allocates the 145 risk factors into six subgroups according to there thirty-year estimated strength.
For each subgroup, we want to investigate how will the Elastic net algorithm, alongside the Lasso regression, select the risk factors for company stock.
We would expect that when facing factor with strong strength, the algorithm will construct a dense factor model, and with the factor strength decrease, the density will decrease simultaneously.

To simplify implementation of the Elastic net, we first run an OLS regression between the market factor and the excess return of each company, and use the resulting residuals as the dependent variable in the Elastic net regression.
This step helps us to remove the potential influence of market factors, allowing the algorithms only focussing on the risk factors. 

Now, the main challenge of applying the Elastic net algorithm is to select the appropriate tuning parameters $\theta$ and $\phi$, and we will discuss the choice of tuning parameter in the following section.

\section{Properties of Risk Factors} \label{EN:risk_factor}

Before we start to discuss the parameter tuning, we briefly review the properties of the risk factors series.
The risk factors data set contains 145 risk factors at the monthly frequency for the period from July 1988 to December 2007.\footnote{For how those factors are constructed, please view \citeA{Feng2020}}
As a standard practice of time series data, we exam the stationarity of the risk factor series by employing Augmented Dick-fuller (ADF) test  \cite{Dickey1979},  Phillips-Perron (PP) test \cite{Phillips1988}, and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test \cite{Kwiatkowski1992}.
Overall, the ADF test and PP test both provides the same conclusions that all 145 risk factor series are unit-root free.
The KPSS test, however, disagrees with the ADF and PP test.
If we take 0.05 as the threshold of p-value, the KPSS test concludes that there are six unit root processes across the 145 factors.
We also present the correlation heat map of all 145 risk factors in the Figure \ref{figure:correlation} at the appendix \ref{factor_correlation}.
All risk factors are sorted base on their estimated factor strength.
The dark lower-left area of the Figure \ref{figure:correlation} indicates that factor with strong estimated strength presents a high correlation with other strong factors.
With the factor strength decrease, the correlation coefficient among factors decreases significantly.
If focusing on the upper-right corner of the figure, where weak factors are clustering, we can see that the correlation coefficients are close to zero.
By observing the upper-left corner and the lower-right corer, we can observe that the correlation between weak factors and strong factors are also very low


\section{Tuning Parameter} \label{EN:parameter_tuning }
To fit the Elastic net, we use the R package \textit{glmnet} \cite{Friedman2010, Simon2011}.

From the equation (\ref{EN:empirical_formula}) and (\ref{EN:elastic_net_penalty}) we know that the estimation of Elastic net using \textit{glmnet} dependent on two tuning parameters $\theta$ and $\phi$.
The \textit{glmnet} package provides function to select the $\phi$ value by adopting the principle of minimise mean squared error (MSE) and ten-fold cross-validation.
%The \textit{glmnet} package provides function to select the $\phi$ automatically. 
%This selection is base on the minimisation of Mean Squared Error (MSE), using 10-fold cross-validation.
However, the package does not provide aid on determining which value of $\theta$ parameter is optimal.
Therefore, we adopt the following strategy to select our tuning parameters $\phi$  and $\theta$:
\begin{enumerate}
\item Prepare a sequence of $\theta$ values, from 0: ridge regression, to 1: Lasso regression with the step of 0.01
\item Randomly assign 90\% of the data set as the training set and the rest 10\% as the test set. 
\item For each of the $\theta$ value, we fit the corresponding Elastic net model using the training set, with function picked $\phi$ values.
\item Base on the $\theta$ and $\phi$ values select, we produce the predicted values using the test data, and calculate the MSE between the true values and predicted values.
\item We select the $\theta - \phi$ combination which minimises the MSE.
\end{enumerate}
We repeat the above procedures 2000 times for each factor strength group and
calculates the $\bar{\theta}$ by averaging every $\theta$ we obtained from the repetition.
% average each $\theta$ values to generate $\bar{\theta}$, as our selected tuning parameter.
Due to the computational burden, when implementing step 2, instead of using the full sample of stocks and factors, we randomly select 50 companies from 242 all companies, and 10 factors from each subgroups.
The selected tuning parameter $\theta$ values are shown in table \ref{table:optimal_theta}.
\begin{table}[H]
	\centering
	\caption{Estimated Optimal $\theta$ values for different factor groups}
	\label{table:optimal_theta}
	\begin{tabular}{l|cccc}
		\hline
		\hline
		Factor Group            & (0, 0.5{]}   & (0.5, 0.6{]} & (0.6, 0.7{]} & (0.7, 0.8{]} \\ 
		Selected $\theta$ value & 0.377        & 0.401        & 0.429        & 0.411        \\ \hline
		Factor Group            & (0.8, 0.9{]} & (0.9, 1{]}   & Mix          & Random       \\ 
		Selected $\theta$ value & 0.396        & 0.413        & 0.448        & 0.431        \\ \hline
		\hline
	\end{tabular}
\end{table}
In order to fully investigate the behaviours of parameter tuning, on the basis of the six subgroups, we create tow addition groups.
The Mix group contain five highly strong factors: factor with strength higher than 0.9, and five weak factors: factor with strength lower than 0.5.
The Random group consist of ten randomly selected factors from all 145 risk factors, and the 10 factors are re-selected every replication.
From table \ref{table:optimal_theta} we can see that the selected $\theta$ value in general increase with the factor strength increase.
For weak factor group, the selected parameter value is 0.377.
While for the strong factor group, the value is 0.413.
The mix factor group has the highest $\theta$ value of 0.448.

%Such a pattern of the $\theta$ value, however, does not follow what we expected.
%The definition of factor strength indicates that factor with strong strength is able to produce more significant loadings, in other words, can explain more assets' risk-return relationship.
%Therefore, when using the above procedure to decide tuning parameter, we would expect that for factor groups with lower strength, like groups with strength smaller than 0.5, the selected $\theta$ parameter will be close to one, or larger than other groups' selected $\theta$.
%This is because factor with weak strength will only provide limited pricing power, and therefore may be recognised by the algorithm as redundant variables.
%When $\theta$ is closer to unity, the elastic net is behaved more like a Lasso, which will eliminate variables provides limited explaining power.
%In contrast, when the group has stronger strength, the $\theta$ will approach closer to zero, leads the Elastic nets more like Ridge, which will not eliminate any variables, but only reduce the coefficient.
%So we would expect the $\theta$ value to increase with the group strength decrease.


The upward pattern of $\theta$ value can be explained by several reasons.
First, MSE may not be an ideal criterion for parameter tuning under the scenario of applying Elastic net with regard to the financial return. 
In our application, we find that the MSE for all $\theta - \phi$ combinations are very close to each other.
The results of MSE for all combination are around 64.
%Therefore, we can not tell the differences among the results of $\theta$ value by using MSE.
So the algorithm can not distinct the results 
Therefore, we can hardly tell the differences of adopting different $\theta$ just by MSE.
Second, because of the estimation method we used, the market risk has already been absorbed by the market factors.
What been left is only the idiosyncratic risk of the assets, and when using risk factors we will expected those factors can form certain linear combination to  explain those risk.
When all factors are strong, the linear combination may consist by a few of factors because even just a single strong factor, we would think it can explain most of the risk.
Then the rest of the factors may be recognised by the algorithm as redundancy and abandoned.
But when we have weak factors, the linear combination may rely on more factor's contribution.
%Then for the strong factors, we would expect that any single of them, or  a very small portion of them, can explained most of the idiosyncratic risk left by the market factor.
%Therefore, when we ask any ten strong factors to determine the risk simultaneously, it is possible that very few of the ten factors can explain most of the risk and the other risk factors will be recognised by the algorithm as redundant.
%But if all factors are weak, it is possible that there exist some linear combinations among most, if not all, weak factors provides enough explaining power for the idiosyncratic risk.
%But if all factors are weak, it is possible that there exists some linear combinations among most, if not all, weak factors, and such combination will provides enough explaining power for the idiosyncratic risk.
Therefore, those weak factors will be reserved by the algorithm, and hence, the parameter $\theta$ will close to zero, indicates a more Ridge like regression.





\section{Elastic Net Findings}
We applied the Elastic net algorithm with tuning parameters estimated in the previous chapter using the thirty-year data set.
The reasons why we use the thirty-year data instead of ten or twenty years is because the Monte Carlo simulation result (see chapter \ref{MC_findings} and Appendix \ref{simulationtable}) indicates us that the estimation accuracy of factor strength will improve significantly when the time-series observation is large.
Therefore, we use the factor strength estimated from the thirty-year data set.

We divided the 145 risk factors into six groups base on their factor strength.
We also randomly selected ten factors from  weak factor group (factor with strength less than 0.5), and ten factors from strong factor group (factor with strength above 0.9) to form a mixed factor group.
For each factor groups, we run two regression: the Elastic net regression with $\theta$ values presented in table \ref{table:optimal_theta}, and the Lasso regression with $\theta = 1$.
Instead of running a pooled regression, we run the Elastic net and Lasso for each individual company, and record the result of factor selection of every stock.
First, we focusing on the general behaviours of factor selection between our two methods.
Table \ref{table:select_prop} presents the average factor selection amount for each factor groups of two selection methods.
\begin{table}[h]
	\centering
		\caption{Average factor selection proportions and factor selection counts of Elastic Net and Lasso}
		\label{table:select_prop}
		\resizebox{\textwidth}{!}{
	\begin{tabular}{l|ccccccc}
		\hline
		\hline
		Factor Group                   & (0,0.5] & (0.5, 0.6]& (0.6, 0.7] & (0.7, 0.8] & (0.8,0.9] & (0.9,1] & Mix \\
		Factor Amount                  & 12            & 10               & 17               & 37               & 35              & 34            & 20  \\ \hline
		Avg EN selection amount        & 2.11          & 4.47             & 8.67             & 14.67            & 13.51           & 12.37         & 8.45                    \\
		Avg EN selection proportion    & 17.5\%        & 44.73\%          & 51.00\%          & 39.65\%          & 38.61\%         & 36.38\%       & 42.28\%                 \\
		Avg Lasso selection amount     & 2.06          & 3.87             & 8.43             & 13               & 12.19           & 10.46         & 7.26                    \\
		Avg Lasso selection proportion & 17.2\%        & 38.76\%          & 49.60\%          & 35.14\%          & 34.83\%         & 30.75\%       & 36.27\%                 \\ \hline\hline
	\end{tabular}
}
\end{table}
We can see that the factor model selected by the Lasso regression, on average, is more parsimonious than the model selected by the Elastic net.
Such discrepancy between our tow methods increases with factor strength increase.
For the weak factor group with strength less than 0.5, the Elastic net and Lasso provides a similar answer: only two factors are selected out of twelve candidates.
While when focusing factor with strength above 0.9, Lasso will select almost 2 fewer factors than the Elastic net.
This result is not surprising, since the tuning parameter $\theta$ of Elastic net is closer to 0 than 1, which means that the Elastic net algorithm in our application tend to keep the factors even though they can provide very limited information.
Unexpectedly, the proportion of factor selection to the strong-factor group is significantly lower than the semi-strong group.
For the factor group with strength between 0.6 and 0.7, both Elastic net and Lasso select almost half of 17 candidates factors. 
But when facing strong factors with strength above 0.9, Elastic net on average only keep about 36\% factors.


We then compare every single stock's factor selecting decision made by Lasso and Elastic net, and calculates in what degree those two methods will agree with each other.
For every single company, if both Elastic net and Lasso select the same factor (generates factor loading not equals to zero), and disregard the same factor (generate factor loadings equal to zero), we call Elastic net and Lasso made an exact agreement of factor selecting.
%We also lower our comparison standard of the agreement to 90\% level. 
%If the Elastic net and Lasso achieve agreements in 90\% of factors selecting decision, and opposite for 10\% of factors, we would call they made an agreement on 90\% level.
%This step helps us to understand their selecting decision making from a broader view.
The proportion of agreement is presented in the table \ref{table:proportion_agreement}.

\begin{table}[h]
	\centering
	\caption{Proportion of Lasso Regression and Elastic Net produces same results for 145 companies}
	\label{table:proportion_agreement}
	\begin{tabular}{c|ccccccc}
		\hline
		\hline
		\multicolumn{1}{l|}{Factor Group}                                         & \multicolumn{1}{l}{(0,0.5{]}} & \multicolumn{1}{l}{(0.5, 0.6{]}} & \multicolumn{1}{l}{(0.6, 0.7{]}} & \multicolumn{1}{l}{(0.7, 0.8{]}} & \multicolumn{1}{l}{(0.8,0.9{]}} & \multicolumn{1}{l}{(0.9,1{]}} & Mix    \\ \hline
		\begin{tabular}[c]{@{}c@{}}Proportion of Agreement\\ (Exact)\end{tabular} & 68.7\%                        & 55.9\%                           & 42.8\%                           & 20.9\%                           & 17.7\%                          & 13.9\%                        & 34.6\% \\ \hline \hline
%		\begin{tabular}[c]{@{}c@{}}Proportion of Agreement\\ (90\%)\end{tabular}  & 86.8\%                        & 72.0\%                           & 74.5\%                           & 72.0\%                           & 79.8\%                          & 74.4\%                        & 76.1\% \\ \hline\hline
	\end{tabular}
\end{table}

It is clear that the proportion of agreement, both exact agreed and 90\% agreed, shows a decreasing trend with the factor strength increase.
Nearly 70\% of factors selection results are identical in the 0 to 0.5 strength group, but this number will decrease to 55\% after we move to the 0.5-0.6 factor strength group.
Only 14\% of companies have identical factor selection results for group with strength between 0.9 and 1.
For the mixed strength group, the exact agreed proportion is 34.6\%, ranked between the 0.6 to 0.7 group and 0.7 to 0.8 group.
%If we focusing on the 90\% agreement column, we can see that under a looser condition, Elastic net and Lasso had made more similar selecting decision.
%For the weak factor group, over 85\% stocks' factor selecting decision made by Elastic net and Lasso are close to each other.

To provide an explanation of the disagreement increasing with factor strength increasing, we calculate the average absolute correlation coefficient among factors for each factor groups.
For every two factors, we calculate the Pearson correlation coefficient, taking the absolute values and average all coefficients.
The result is presented in table \ref{table:Correlation} and Figure \ref{figure:correlation}.
\begin{table}[h]
	\centering
	\caption{Correlation Coefficient among different factor groups. }
	\label{table:Correlation}
	\begin{tabular}{l|cccccc}
		\hline
		\hline
		Factor Group                                 & (0,0.5{]} & (0.5, 0.6{]} & (0.6, 0.7{]} & (0.7, 0.8{]} & (0.8,0.9{]} & (0.9,1{]} \\ \hline
		\multicolumn{1}{c|}{Correlation Coefficient} & 0.0952    & 0.157        & 0.213        & 0.229        & 0.371       & 0.724   \\
		Factor Amount &12 & 10 &  17 & 37& 35 &34  \\ \hline \hline
	\end{tabular}
\end{table}
We can clearly see an increasing trend of the correlation coefficient with the increase of factor's strength.
This correlation pattern provides a possible explanation of the discord results of Elastic net and Lasso.
When facing variables with high correlation, Lasso will randomly select several variables and discard others \cite{Kozak2020}.
While Elastic net address this problem with the help of the extra $L^2$ norm in its loss function.
Therefore, the Elastic net method will select all factors that can bring new information to explain the risk-return relationship even those factors are highly correlated.
This also provides a potential explanation to what we observed in the table \ref{table:proportion}  that Lasso selects significantly fewer factors than the Elastic net method.
Also, recall the fact that in the strong factor group, Elastic net will select two more factors than the Lasso on average.
We believe Elastic net here will pick up factors that been abandoned by the Lasso due to the correlativity.
