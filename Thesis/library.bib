@article{Anatolyev2018,
   abstract = {This paper re-examines the problem of estimating risk premia in linear factor pricing models. Typically, the data used in the empirical literature are characterized by weakness of some pricing factors, strong cross-sectional dependence in the errors, and (moderately) high cross-sectional dimensionality. Using an asymptotic framework where the number of assets/portfolios grows with the time span of the data while the risk exposures of weak factors are local-to-zero, we show that the conventional two-pass estimation procedure delivers inconsistent estimates of the risk premia. We propose a new estimation procedure based on sample-splitting instrumental variables regression. The proposed estimator of risk premia is robust to weak included factors and to the presence of strong unaccounted cross-sectional error dependence. We derive the many-asset weak factor asymptotic distribution of the proposed estimator, show how to construct its standard errors, verify its performance in simulations, and revisit some empirical studies.},
   author = {Stanislav Anatolyev and Anna Mikusheva},
   journal = {CESifo Working Paper Series},
   month = {7},
   title = {Factor models with many assets: strong factors, weak factors, and the two-pass procedure},
   url = {http://arxiv.org/abs/1807.04094},
   year = {2018},
}
@article{Harvey2019,
   abstract = {The rate of factor production in the academic research is out of control. We document over 400 factors published in top journals. Surely, many of them are false. We explore the incentives that lead to factor mining and explore reasons why many of the factors are simply lucky findings. The backtested results published in academic outlets are routinely cited to support commercial products. As a consequence, investors develop exaggerated expectations based on inflated backtested results and are then disappointed by the live trading experience. We provide a comprehensive census of factors published in top academic journals through January 2019. We also offer a link to a Google sheet that has detailed information on each factor, including citation information and download links. Finally, we propose a citizen science project that allows researchers to add to our database both published papers as well as working papers.},
   author = {Campbell R. Harvey and Yan Liu},
   doi = {10.2139/ssrn.3341728},
   journal = {SSRN Electronic Journal},
   keywords = {Backtesting,Data Mining,Factor Investing,Momentum,Multiple Testing,Overfitting,Value Investing},
   month = {3},
   publisher = {Elsevier BV},
   title = {A Census of the Factor Zoo},
   year = {2019},
}
@working_paper{Feng2019,
   author = {Guanhao Feng and Stefano Giglio and Dacheng Xiu},
   city = {Cambridge, MA},
   doi = {10.3386/w25481},
   institution = {National Bureau of Economic Research},
   month = {1},
   title = {Taming the Factor Zoo: A Test of New Factors},
   url = {http://www.nber.org/papers/w25481.pdf},
   year = {2019},
}
@article{Bailey2016,
   abstract = {This paper provides a characterisation of the degree of cross-sectional dependence in a two dimensional array, \{xit,i = 1,2,..N;t = 1,2,..,T\} in terms of the rate at which the variance of the cross-sectional average of the observed data varies with N. Under certain conditions this is equivalent to the rate at which the largest eigenvalue of the covariance matrix of xt=(x1t,x2t,..,xNt)′ rises with N. We represent the degree of cross-sectional dependence by α, which we refer to as the ‘exponent of cross-sectional dependence’, and define it by the standard deviation, Std((Formula presented.))= O (Nα-1), where (Formula presented.) is a simple cross-sectional average of xit. We propose bias corrected estimators, derive their asymptotic properties for α > 1/2 and consider a number of extensions. We include a detailed Monte Carlo simulation study supporting the theoretical results. We also provide a number of empirical applications investigating the degree of inter-linkages of real and financial variables in the global economy. Copyright © 2015 John Wiley & Sons, Ltd.},
   author = {Natalia Bailey and George Kapetanios and M. Hashem Pesaran},
   doi = {10.1002/jae.2476},
   issn = {08837252},
   issue = {6},
   journal = {Journal of Applied Econometrics},
   month = {9},
   pages = {929-960},
   publisher = {John Wiley and Sons Ltd},
   title = {Exponent of Cross-Sectional Dependence: Estimation and Inference},
   volume = {31},
   url = {http://doi.wiley.com/10.1002/jae.2476},
   year = {2016},
}
@article{Bailey2020,
   abstract = {This paper proposes an estimator of factor strength and establishes its consistency and asymp-totic distribution. The proposed estimator is based on the number of statistically significant factor loadings, taking account of the multiple testing problem. We focus on the case where the factors are observed which is of primary interest in many applications in macroeconomics and finance. We also consider using cross section averages as a proxy in the case of unobserved common factors. We face a fundamental factor identification issue when there are more than one unobserved common factors. We investigate the small sample properties of the proposed estimator by means of Monte Carlo experiments under a variety of scenarios. In general, we find that the estimator, and the associated inference, perform well. The test is conservative under the null hypothesis, but, nevertheless, has excellent power properties, especially when the factor strength is sufficiently high. Application of the proposed estimation strategy to factor models of asset returns shows that out of 146 factors recently considered in the finance literature, only the market factor is truly strong, while all other factors are at best semi-strong, with their strength varying considerably over time. Similarly, we only find evidence of semi-strong factors in an updated version of the Stock and Watson (2012) macroeconomic dataset.},
   author = {Natalia Bailey and George Kapetanios and M. Hashem Pesaran},
   journal = {CESifo Working Paper},
   keywords = {E20,Factor models,G20,cross-sectional depen-dence,factor strength,market factor JEL Classifications: C38,measures of pervasiveness},
   title = {Measurement of Factor Strength: Theory and Practice},
   year = {2020},
}
@article{Sarafidis2012,
   abstract = {This article provides an overview of the existing literature on panel data models with error cross-sectional dependence (CSD). We distinguish between weak and strong CSD and link these concepts to the spatial and factor structure approaches. We consider estimation under strong and weak exogeneity of the regressors for both T fixed and T large cases. Available tests for CSD and methods for determining the number of factors are discussed in detail. The finite-sample properties of some estimators and statistics are investigated using Monte Carlo experiments. © 2012 Copyright Taylor and Francis Group, LLC.},
   author = {Vasilis Sarafidis and Tom Wansbeek},
   doi = {10.1080/07474938.2011.611458},
   issn = {0747-4938},
   issue = {5},
   journal = {Econometric Reviews},
   keywords = {Cross-sectional dependence,Factor structure,Panel data,Spatial dependence,Strong/Weak exogeneity},
   month = {9},
   pages = {483-531},
   publisher = { Taylor & Francis Group },
   title = {Cross-Sectional Dependence in Panel Data Analysis},
   volume = {31},
   url = {https://www.tandfonline.com/doi/full/10.1080/07474938.2011.611458},
   year = {2012},
}
@article{Chamberlain1983,
   abstract = {We present a definition of factor structure that is less restrictive than the one typically used in arbitrage pricing models. Our factor structure restrictions build on the following intuitive distinctions between factor variance and idiosyncratic variance: (i) A well-diversified portfolio contains only factor variance. (ii) If a portfolio is uncorrelated with the well-diversified portfolios, then it contains only idiosyncratic variance; so if a sequence of such portfolios becomes well-diversified, the limiting variance should be zero. Our factor structure restrictions imply Ross' [5] arbitrage pricing formula. We obtain upper and lower bounds on the approximation error in that formula; these bounds may be useful in empirical work. They imply that arbitrage pricing is exact if and only if there is a risky, well-diversified portfolio on the mean-variance frontier. If all mean-variance efficient portfolios are well-diversified, then the well-diversified portfolios provide mutual fund separation. Our factor structure restrictions are satisfied (with K factors) if and only if the covariance matrix of asset returns has only K unbounded eigenvalues as the number of assets increases.},
   author = {Gary Chamberlain},
   doi = {10.2307/1912276},
   issn = {00129682},
   issue = {5},
   journal = {Econometrica},
   month = {9},
   pages = {1305},
   publisher = {JSTOR},
   title = {Funds, Factors, and Diversification in Arbitrage Pricing Models},
   volume = {51},
   year = {1983},
}
@article{Chen2011,
   abstract = {A new factor model consisting of the market factor, an investment factor, and a return-on-equity factor is a good start to understanding the cross-section of expected stock returns. Firms will invest a lot when their profitability is high and the cost of capital is low. As such, controlling for profitability, investment should be negatively correlated with expected returns, and controlling for investment, profitability should be positively correlated with expected returns. The new three-factor model reduces the magnitude of the abnormal returns of a wide range of anomalies-based trading strategies, often to insignificance. The model's performance, combined with its economic intuition, suggests that it can be used to obtain expected return estimates in practice.},
   author = {Long Chen and Robert Novy-Marx and Lu Zhang},
   doi = {10.2139/ssrn.1418117},
   journal = {SSRN Electronic Journal},
   keywords = {Alphas,Anomalies,Asset Pricing Tests,Factor Regressions},
   month = {12},
   publisher = {Elsevier BV},
   title = {An Alternative Three-Factor Model},
   year = {2011},
}
@article{Harvey2014,
   abstract = {We propose a new regression method to select amongst a large group of candidate factors - many of which might be the result of data mining - that purport to explain the cross-section of expected returns. The method is robust to general distributional characteristics of both factor and asset returns. We allow for the possibility of time-series as well as cross-sectional dependence. The technique accommodates a wide range of test statistics such as t-ratios. While our main application focuses on asset pricing, the method can be applied in any situation where regression analysis is used in the presence of multiple testing. This includes, for example, the evaluation of investment manager performance as well as time-series prediction of asset returns.},
   author = {Campbell R. Harvey and Yan Liu},
   doi = {10.2139/ssrn.2528780},
   journal = {SSRN Electronic Journal},
   keywords = {Bootstrap,Data mining,Factors,Fama-MacBeth,GRS,Multiple testing,Orthogonalization,Performance evaluation,Predictive regressions,Return prediction,Variable selection},
   month = {11},
   publisher = {Elsevier BV},
   title = {Lucky Factors},
   year = {2014},
}
@article{Harvey2015,
   abstract = {Hundreds of papers and factors attempt to explain the cross-section of expected returns. Given this extensive data mining, it does not make sense to use the usual criteria for establishing significance. Which hurdle should be used for current research? Our paper introduces a new multiple testing framework and provides historical cutoffs from the first empirical tests in 1967 to today. A new factor needs to clear a much higher hurdle, with a t-statistic greater than 3.0. We argue that most claimed research findings in financial economics are likely false.Received October 22, 2014; accepted June 15, 2015 by Editor Andrew Karolyi.},
   author = {Campbell R Harvey and Yan Liu and Heqing Zhu},
   doi = {10.1093/rfs/hhv059},
   issn = {0893-9454},
   issue = {1},
   journal = {The Review of Financial Studies},
   month = {10},
   pages = {5-68},
   title = {… and the Cross-Section of Expected Returns},
   volume = {29},
   url = {https://doi.org/10.1093/rfs/hhv059},
   year = {2015},
}
@article{Bailey2019,
   abstract = {This paper proposes a regularisation method for the estimation of large covariance matrices that uses insights from the multiple testing (MT) literature. The approach tests the statistical significance of individual pair-wise correlations and sets to zero those elements that are not statistically significant, taking account of the multiple testing nature of the problem. The effective p-values of the tests are set as a decreasing function of N (the cross section dimension), the rate of which is governed by the nature of dependence of the underlying observations, and the relative expansion rates of N and T (the time dimension). In this respect, the method specifies the appropriate thresholding parameter to be used under Gaussian and non-Gaussian settings. The MT estimator of the sample correlation matrix is shown to be consistent in the spectral and Frobenius norms, and in terms of support recovery, so long as the true covariance matrix is sparse. The performance of the proposed MT estimator is compared to a number of other estimators in the literature using Monte Carlo experiments. It is shown that the MT estimator performs well and tends to outperform the other estimators, particularly when N is larger than T.},
   author = {Natalia Bailey and M. Hashem Pesaran and L. Vanessa Smith},
   doi = {10.1016/j.jeconom.2018.10.006},
   issn = {18726895},
   issue = {2},
   journal = {Journal of Econometrics},
   keywords = {High-dimensional data,Multiple testing,Non-Gaussian observations,Shrinkage,Sparsity,Thresholding},
   month = {2},
   pages = {507-534},
   publisher = {Elsevier Ltd},
   title = {A multiple testing approach to the regularisation of large sample correlation matrices},
   volume = {208},
   year = {2019},
}
@article{Chudik2018,
   abstract = {© 2018 The Econometric Society This paper provides an alternative approach to penalized regression for model selection in the context of high-dimensional linear regressions where the number of covariates is large, often much larger than the number of available observations. We consider the statistical significance of individual covariates one at a time, while taking full account of the multiple testing nature of the inferential problem involved. We refer to the proposed method as One Covariate at a Time Multiple Testing (OCMT) procedure, and use ideas from the multiple testing literature to control the probability of selecting the approximating model, the false positive rate, and the false discovery rate. OCMT is easy to interpret, relates to classical statistical analysis, is valid under general assumptions, is faster to compute, and performs well in small samples. The usefulness of OCMT is also illustrated by an empirical application to forecasting U.S. output growth and inflation.},
   author = {A. Chudik and G. Kapetanios and M. Hashem Pesaran},
   doi = {10.3982/ecta14176},
   issn = {0012-9682},
   issue = {4},
   journal = {Econometrica},
   keywords = {Monte Carlo experiments,One covariate at a time,boosting,high dimensionality,model selection,multiple testing,penalized regressions},
   month = {7},
   pages = {1479-1512},
   publisher = {The Econometric Society},
   title = {A One Covariate at a Time, Multiple Testing Approach to Variable Selection in High-Dimensional Linear Regression Models},
   volume = {86},
   year = {2018},
}
@article{Pukthuanthong2019,
   abstract = {We propose a protocol for identifying genuine risk factors. A genuine risk factor must be related to the covariance matrix of returns, must be priced in the cross-section of returns, and should yield a reward-to-risk ratio that is reasonable enough to be consistent with risk pricing. A market factor, a profitability factor, and traded versions of macroeconomic factors pass our protocol, but many characteristic-based factors do not. Several of the underlying characteristics, however, do command premiums in the cross-section.},
   author = {Kuntara Pukthuanthong and Richard Roll and Avanidhar Subrahmanyam},
   doi = {10.1093/rfs/hhy093},
   issn = {14657368},
   issue = {4},
   journal = {Review of Financial Studies},
   month = {8},
   pages = {1573-1607},
   title = {A protocol for factor identification},
   volume = {32},
   url = {https://doi.org/10.1093/rfs/hhy093},
   year = {2019},
}
@article{Uematsu2019,
   abstract = {In this paper, we propose a novel consistent estimation method for the approximate factor model of Chamberlain and Rothschild (1983), with large cross-sectional and time-series dimensions (N and T, respectively). Their model assumes that the r (≪N) largest eigenvalues of data covariance matrix grow as N rises without specifying each diverging rate. This is weaker than the typical assumption on the recent factor models, in which all the r largest eigenvalues diverge proportionally to N, and is frequently referred to as the weak factor models. We extend the sparse orthogonal factor regression (SOFAR) proposed by Uematsu et al. (2019) to consider consistent estimation of the weak factors structure, where the k-th largest eigenvalue grows proportionally to N^\{α_\{k\}\} with some unknown exponents 0},
   author = {Yoshimasa Uematsu and Takashi Yamagata},
   journal = {ISER Discussion Paper},
   publisher = {Institute of Social and Economic Research, Osaka University},
   title = {Estimation of Weak Factor Models},
   year = {2019},
}
@article{Pesaran2019,
   abstract = {In this paper we are concerned with the role of factor strength and pricing errors in asset pricing models, and their implications for identification and estimation of risk premia. We establish an explicit relationship between the pricing errors and the presence of weak factors that are correlated with stochastic discount factor. We introduce a measure of factor strength, and distinguish between observed factors and unobserved factors. We show that unobserved factors matter for pricing if they are correlated with the discount factor, and relate the strength of the weak factors to the strength (pervasiveness) of non-zero pricing errors. We then show, that even when the factor loadings are known, the risk premia of a factor can be consistently estimated only if it is strong and if the pricing errors are weak. Similar results hold when factor loadings are estimated, irrespective of whether individual returns or portfolio returns are used. We derive distributional results for two pass estimators of risk premia, allowing for non-zero pricing errors. We show that for inference on risk premia the pricing errors must be sufficiently weak. We consider both when n (the number of securities) is large and T (the number of time periods) is short, and the case of large n and T. Large n is required for consistent estimation of risk premia, whereas the choice of short T is intended to reduce the possibility of time variations in the factor loadings. We provide monthly rolling estimates of the factor strengths for the three Fama-French factors over the period 1989-2018.},
   author = {M. Hashem Pesaran and Ron P. Smith},
   journal = {CESifo Working Paper Series},
   keywords = {APT,Fama-French factors,arbitrage pricing theory,factor strength,identification of risk premia,two-pass regressions},
   publisher = {CESifo Group Munich},
   title = {The Role of Factor Strength and Pricing Errors for Estimation and Inference in Asset Pricing Models},
   year = {2019},
}
@article{McLean2016,
   abstract = {We study the out-of-sample and post-publication return predictability of 97 variables shown to predict cross-sectional stock returns. Portfolio returns are 26% lower out-of-sample and 58% lower post-publication. The out-of-sample decline is an upper bound estimate of data mining effects. We estimate a 32% (58%-26%) lower return from publication-informed trading. Post-publication declines are greater for predictors with higher in-sample returns, and returns are higher for portfolios concentrated in stocks with high idiosyncratic risk and low liquidity. Predictor portfolios exhibit post-publication increases in correlations with other published-predictor portfolios. Our findings suggest that investors learn about mispricing from academic publications.},
   author = {R. David McLean and Jeffrey Pontiff},
   doi = {10.1111/jofi.12365},
   issn = {00221082},
   issue = {1},
   journal = {The Journal of Finance},
   month = {2},
   pages = {5-32},
   publisher = {Blackwell Publishing Ltd},
   title = {Does Academic Research Destroy Stock Return Predictability?},
   volume = {71},
   url = {http://doi.wiley.com/10.1111/jofi.12365},
   year = {2016},
}
@article{Harvey2017,
   abstract = {Investors make two types of mistakes. First, they erroneously allocate to an asset manager (or a “smart” beta) that underperforms because the asset manager lacks skill. Second, investors might miss out allocating to a good manager. The first mistake is difficult to deal with given there are thousands of managers and many look good purely by luck. We introduce a new technique that optimizes the threshold for a prespecified false discovery rate (i.e., chance of the first mistake), at say 5%. Our method also allows for heterogeneous false discoveries – we should not treat all bad managers the same because some are really, really bad. Next, we focus on the second type of error where investors miss out on good managers. It is routine to ignore this type of mistake. Our results show that current research methods have little or no power to detect good managers. Finally, our method allows for the asymmetric treatment of false discoveries and misses – generally, investing in a bad manager is more costly than missing a good manager. We also offer a way to select managers whereby the investor can prespecify the ratio of false discoveries to misses to accommodate these differential costs. For instance, we can accommodate a decision rule whereby the investor is willing to miss ten good managers to avoid the mistake of selecting one bad manager.},
   author = {Campbell R. Harvey and Yan Liu},
   doi = {10.2139/ssrn.3073799},
   journal = {SSRN Electronic Journal},
   keywords = {Anomalies,Backtesting,Bayesian,Factor Zoo,Factors,False discoveries,Multiple testing,Mutual funds,Odds ratio,Power,Smart beta,Type I,Type II},
   month = {12},
   publisher = {Elsevier BV},
   title = {False (and Missed) Discoveries in Financial Economics},
   year = {2017},
}
@book{Hastie2009,
   abstract = {"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates."--Publisher's website.},
   author = {Trevor author Hastie},
   city = {New York, N.Y.},
   edition = {2nd editio},
   editor = {Robert Tibshirani  author and Jerome Friedman  author},
   keywords = {Bioinformatics,Computational Biology,Computational intelligence,Data Mining,Data mining,EstatiМЃstica,EstatiМЃstica computacional,Forecasting,Inference,InfereМ‚ncia estatiМЃstica,Machine learning,Mathematical Computing,MineracМ§aМѓo de dados,Statistics -- Methodology,Statistics as Topic,Supervised learning (Machine learning)},
   publisher = {New York, N.Y. : Springer},
   title = {The elements of statistical learning : data mining, inference and prediction},
   year = {2009},
}
@article{Sharpe1964,
   author = {William F. Sharpe},
   doi = {10.1111/j.1540-6261.1964.tb02865.x},
   issn = {00221082},
   issue = {3},
   journal = {The Journal of Finance},
   month = {9},
   pages = {425-442},
   publisher = {John Wiley & Sons, Ltd},
   title = {CAPITAL ASSET PRICES: A THEORY OF MARKET EQUILIBRIUM UNDER CONDITIONS OF RISK},
   volume = {19},
   url = {http://doi.wiley.com/10.1111/j.1540-6261.1964.tb02865.x},
   year = {1964},
}
@article{Black1972,
   author = {Fischer Black},
   issn = {00219398, 15375374},
   issue = {3},
   journal = {The Journal of Business},
   pages = {444-455},
   publisher = {University of Chicago Press},
   title = {Capital Market Equilibrium with Restricted Borrowing},
   volume = {45},
   url = {www.jstor.org/stable/2351499},
   year = {1972},
}
@article{Fama1992,
   abstract = {Two easily measured variables, size and book‐to‐market equity, combine to capture the cross‐sectional variation in average stock returns associated with market β, size, leverage, book‐to‐market equity, and earnings‐price ratios. Moreover, when the tests allow for variation in β that is unrelated to size, the relation between market β and average return is flat, even when β is the only explanatory variable. 1992 The American Finance Association},
   author = {Eugene F. Fama and Kenneth R. French},
   doi = {10.1111/j.1540-6261.1992.tb04398.x},
   issn = {00221082},
   issue = {2},
   journal = {The Journal of Finance},
   month = {6},
   pages = {427-465},
   publisher = {John Wiley & Sons, Ltd},
   title = {The Cross-Section of Expected Stock Returns},
   volume = {47},
   url = {http://doi.wiley.com/10.1111/j.1540-6261.1992.tb04398.x},
   year = {1992},
}
@article{Lintner1965,
   author = {John Lintner},
   doi = {10.2307/1924119},
   issn = {00346535},
   issue = {1},
   journal = {The Review of Economics and Statistics},
   keywords = {Business -- Business administration -- Business management,Business -- Business operations -- Economic utility,Business -- Business structures -- Commerce,Economics -- Economic disciplines -- Financial economics,Economics -- Microeconomics -- Economic utility},
   pages = {13-37},
   title = {The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets},
   volume = {47},
   year = {1965},
}
@article{Harvey2017,
   abstract = {Indirect incentives exist in the money management industry when good current performance increases future inflows of new capital, leading to higher future fees. We quantify the magnitude of indirect performance incentives for hedge fund managers. Flows respond quickly and strongly to performance; lagged performance has a monotonically decreasing impact on flows as lags increase up to two years. Conservative estimates indicate that indirect incentives for the average fund are four times as large as direct incentives from incentive fees and returns to managers’ own investment in the fund. For new funds, indirect incentives are seven times as large as direct incentives. Combining direct and indirect incentives, for each dollar generated for their investors in a given year, managers receive close to 75 cents in direct performance fees plus the present value of future fees over the expected life of the fund. Older and capacity constrained funds have considerably weaker relations between future flows and performance, leading to weaker indirect incentives. There is no evidence that direct contractual incentives are stronger when market-based indirect incentives are weaker. Contact},
   author = {Campbell R. Harvey},
   doi = {10.2139/ssrn.2893930},
   journal = {SSRN Electronic Journal},
   keywords = {Bayesian P-values,Data dredging,Data mining,MBF,Minimum Bayes Factor,Multiple testing,P-hacking,P-values,Rare incidence,SD-MBF,Selection,Type I error,Type II error},
   month = {1},
   publisher = {Elsevier BV},
   title = {The Scientific Outlook in Financial Economics},
   year = {2017},
}
@article{Kleibergen2009,
   abstract = {We show that statistical inference on the risk premia in linear factor models that is based on the Fama-MacBeth (FM) and generalized least squares (GLS) two-pass risk premia estimators is misleading when the β's are small and/or the number of assets is large. We propose novel statistics, that are based on the maximum likelihood estimator of Gibbons [Gibbons, M., 1982. Multivariate tests of financial models: A new approach. Journal of Financial Economics 10, 3-27], which remain trustworthy in these cases. The inadequacy of the FM and GLS two-pass t/Wald statistics is highlighted in a power and size comparison using quarterly portfolio returns from Lettau and Ludvigson [Lettau, M., Ludvigson, S., 2001. Resurrecting the (C)CAPM: A cross-sectional test when risk premia are time-varying. Journal of Political Economy 109, 1238-1287]. The power and size comparison shows that the FM and GLS two-pass t/Wald statistics can be severely size distorted. The 95% confidence sets for the risk premia in the above-cited work that result from the novel statistics differ substantially from those that result from the FM and GLS two-pass t-statistics. They show support for the human capital asset pricing model although the 95% confidence set for the risk premia on labor income growth is unbounded. The 95% confidence sets show no support for the (scaled) consumption asset pricing model, since the 95% confidence set of the risk premia on the scaled consumption growth consists of the whole real line, but do not reject it either. © 2009 Elsevier B.V. All rights reserved.},
   author = {Frank Kleibergen},
   doi = {10.1016/j.jeconom.2009.01.013},
   issn = {03044076},
   issue = {2},
   journal = {Journal of Econometrics},
   keywords = {Consumption capital asset pricing model,Size distortion of test statistics,Small β's},
   month = {4},
   pages = {149-173},
   publisher = {North-Holland},
   title = {Tests of risk premia in linear factor models},
   volume = {149},
   year = {2009},
}
@article{Gospodinov2017,
   abstract = {This note studies some seemingly anomalous results that arise in possibly misspec- ified, reduced-rank linear asset-pricing models estimated by the continuously updated generalized method of moments. When a spurious factor (that is, a factor that is un- correlated with the returns on the test assets) is present, the test for correct model specification has asymptotic power that is equal to the nominal size. In other words, applied researchers will erroneously conclude that the model is correctly specified even when the degree of misspecification is arbitrarily large. The rejection probability of the test for overidentifying restrictions typically decreases further in underidentified mod- els where the dimension of the null space is larger than 1.},
   author = {Nikolay Gospodinov and Raymond Kan and Cesare Robotti},
   doi = {10.3982/ecta13750},
   issn = {0012-9682},
   issue = {5},
   journal = {Econometrica},
   keywords = {Asset pricing,continuously updated GMM,model misspecification,rank test,reduced‐rank models,spurious risk factors,test for overidentifying restrictions},
   month = {9},
   pages = {1613-1628},
   publisher = {The Econometric Society},
   title = {Spurious Inference in Reduced-Rank Asset-Pricing Models},
   volume = {85},
   year = {2017},
}
@article{Kan1999,
   abstract = {In this paper we investigate the properties of the standard two-pass methodology of testing beta pricing models with misspecified factors. In a setting where a factor is useless, defined as being independent of all the asset returns, we provide theoretical results and simulation evidence that the second-pass cross-sectional regression tends to find the beta risk of the useless factor priced more often than it should. More surprisingly, this misspecification bias exacerbates when the number of time series observations increases. Possible ways of detecting useless factors are also examined.},
   author = {Raymond Kan and Chu Zhang},
   doi = {10.1111/0022-1082.00102},
   issn = {00221082},
   issue = {1},
   journal = {The Journal of Finance},
   month = {2},
   pages = {203-235},
   publisher = {Blackwell Publishing Inc.},
   title = {Two-Pass Tests of Asset Pricing Models with Useless Factors},
   volume = {54},
   url = {http://doi.wiley.com/10.1111/0022-1082.00102},
   year = {1999},
}
@article{Kleibergen2015,
   abstract = {We construct the large sample distributions of the OLS and GLS R2's of the second pass regression of the Fama and MacBeth (1973) two pass procedure when the observed proxy factors are minorly correlated with the true unobserved factors. This implies an unexplained factor structure in the first pass residuals and, consequently, a large estimation error in the estimated beta's which is spanned by the beta's of the unexplained true factors. The average portfolio returns and the estimation error of the estimated beta's are then both linear in the beta's of the unobserved true factors which leads to possibly large values of the OLS R2 of the second pass regression. These large values of the OLS R2 are not indicative of the strength of the relationship. Our results question many empirical findings that concern the relationship between expected portfolio returns and (macro-) economic factors.},
   author = {Frank Kleibergen and Zhaoguo Zhan},
   doi = {10.1016/j.jeconom.2014.11.006},
   issn = {18726895},
   issue = {1},
   journal = {Journal of Econometrics},
   keywords = {(Non-standard) Large sample distribution,Factor pricing,Fama-MacBeth two pass procedure,Principal components,Stochastic discount factors,Weak identification},
   month = {11},
   pages = {101-116},
   publisher = {Elsevier Ltd},
   title = {Unexplained factors and their effects on second pass R-squared's},
   volume = {189},
   year = {2015},
}
@article{Fama1973,
   abstract = {This paper tests the relationship between average return and risk for New York Stock Exchange common stocks. The theoretical basis of the tests is the "two-parameter" portfolio model and models of market equilibrium derived from the two-parameter portfolio model. We can- not reject the hypothesis of these models that the pricing of common stocks reflects the attempts of risk-averse investors to hold portfolios that are "efficient" in terms of expected value and dispersion of return. Moreover, the observed "fair game" properties of the coefficients and residuals of the risk-return regressions are consistent with an "efficient capital market"-that is, a market where prices of securities fully reflect available information. I.},
   author = {Eugene F. Fama and James D. MacBeth},
   doi = {10.1086/260061},
   issn = {0022-3808},
   issue = {3},
   journal = {Journal of Political Economy},
   month = {5},
   pages = {607-636},
   publisher = {University of Chicago Press},
   title = {Risk, Return, and Equilibrium: Empirical Tests},
   volume = {81},
   year = {1973},
}
@article{Asness2013,
   abstract = {We find consistent value and momentum return premia across eight diverse markets and asset classes, and a strong common factor structure among their returns. Value and momentum returns correlate more strongly across asset classes than passive exposures to the asset classes, but value and momentum are negatively correlated with each other, both within and across asset classes. Our results indicate the presence of common global risks that we characterize with a three-factor model. Global funding liquidity risk is a partial source of these patterns, which are identifiable only when examining value and momentum jointly across markets. Our findings present a challenge to existing behavioral, institutional, and rational asset pricing theories that largely focus on U.S. equities. © 2013 the American Finance Association.},
   author = {Cclifford S. Asness and Tobias J. Moskowitz and Lasse Heje Pedersen},
   doi = {10.1111/jofi.12021},
   issn = {00221082},
   issue = {3},
   journal = {The Journal of Finance},
   month = {6},
   pages = {929-985},
   publisher = {John Wiley & Sons, Ltd},
   title = {Value and Momentum Everywhere},
   volume = {68},
   url = {http://doi.wiley.com/10.1111/jofi.12021},
   year = {2013},
}
@article{Barillas2018,
   abstract = {A Bayesian asset pricing test is derived that is easily computed in closed form from the standard F-statistic. Given a set of candidate traded factors, we develop a related test procedure that permits the computation of model probabilities for the collection of all possible pricing models that are based on subsets of the given factors. We find that the recent models of Hou, Xue, and Zhang (2015a, 2015b) and Fama and French (2015, 2016) are dominated by a variety of models that include a momentum factor, along with value and profitability factors that are updated monthly.},
   author = {Francisco Barillas and Jay Shanken},
   doi = {10.1111/jofi.12607},
   issn = {00221082},
   issue = {2},
   journal = {The Journal of Finance},
   month = {4},
   pages = {715-754},
   publisher = {Blackwell Publishing Ltd},
   title = {Comparing Asset Pricing Models},
   volume = {73},
   url = {http://doi.wiley.com/10.1111/jofi.12607},
   year = {2018},
}
@article{Cochrane2011,
   abstract = {Discount-rate variation is the central organizing question of current asset-pricing research. I survey facts, theories, and applications. Previously, we thought returns were unpredictable, with variation in price-dividend ratios due to variation in expected cashflows. Now it seems all price-dividend variation corresponds to discount-rate variation. We also thought that the cross-section of expected returns came from the CAPM. Now we have a zoo of new factors. I categorize discount-rate theories based on central ingredients and data sources. Incorporating discount-rate variation affects finance applications, including portfolio theory, accounting, cost of capital, capital structure, compensation, and macroeconomics. © 2011 the American Finance Association.},
   author = {John H. Cochrane},
   doi = {10.1111/j.1540-6261.2011.01671.x},
   issn = {00221082},
   issue = {4},
   journal = {The Journal of Finance},
   month = {8},
   pages = {1047-1108},
   publisher = {John Wiley & Sons, Ltd},
   title = {Presidential Address: Discount Rates},
   volume = {66},
   url = {http://doi.wiley.com/10.1111/j.1540-6261.2011.01671.x},
   year = {2011},
}
@article{Yuan2006,
   abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods. © 2006 Royal Statistical Society.},
   author = {Ming Yuan and Yi Lin},
   doi = {10.1111/j.1467-9868.2005.00532.x},
   issn = {1369-7412},
   issue = {1},
   journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
   keywords = {Analysis of variance,Lasso,Least angle regression,Non-negative garrotte,Piecewise linear solution path},
   month = {2},
   pages = {49-67},
   title = {Model selection and estimation in regression with grouped variables},
   volume = {68},
   url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00532.x},
   year = {2006},
}
@article{Tibshirani1996,
   abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
   author = {Robert Tibshirani},
   doi = {10.1111/j.2517-6161.1996.tb02080.x},
   issn = {00359246},
   issue = {1},
   journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
   keywords = {quadratic programming,regression,shrinkage,subset selection},
   month = {1},
   pages = {267-288},
   publisher = {Wiley},
   title = {Regression Shrinkage and Selection Via the Lasso},
   volume = {58},
   url = {http://doi.wiley.com/10.1111/j.2517-6161.1996.tb02080.x},
   year = {1996},
}
@article{Kozak2020,
   abstract = {We construct a robust stochastic discount factor (SDF) summarizing the joint explanatory power of a large number of cross-sectional stock return predictors. Our method achieves robust out-of-sample performance in this high-dimensional setting by imposing an economically motivated prior on SDF coefficients that shrinks contributions of low-variance principal components of the candidate characteristics-based factors. We find that characteristics-sparse SDFs formed from a few such factors—e.g., the four- or five-factor models in the recent literature—cannot adequately summarize the cross-section of expected stock returns. However, an SDF formed from a small number of principal components performs well.},
   author = {Serhiy Kozak and Stefan Nagel and Shrihari Santosh},
   doi = {10.1016/j.jfineco.2019.06.008},
   issn = {0304405X},
   issue = {2},
   journal = {Journal of Financial Economics},
   keywords = {Cross section,Factor models,Machine learning,SDF,Shrinkage},
   month = {2},
   pages = {271-292},
   publisher = {Elsevier B.V.},
   title = {Shrinking the cross-section},
   volume = {135},
   year = {2020},
}
@article{Kelly2019,
   abstract = {We propose a new modeling approach for the cross section of returns. Our method, Instrumented Principal Component Analysis (IPCA), allows for latent factors and time-varying loadings by introducing observable characteristics that instrument for the unobservable dynamic loadings. If the characteristics/expected return relationship is driven by compensation for exposure to latent risk factors, IPCA will identify the corresponding latent factors. If no such factors exist, IPCA infers that the characteristic effect is compensation without risk and allocates it to an “anomaly” intercept. Studying returns and characteristics at the stock-level, we find that five IPCA factors explain the cross section of average returns significantly more accurately than existing factor models and produce characteristic-associated anomaly intercepts that are small and statistically insignificant. Furthermore, among a large collection of characteristics explored in the literature, only ten are statistically significant at the 1% level in the IPCA specification and are responsible for nearly 100% of the model's accuracy.},
   author = {Bryan T. Kelly and Seth Pruitt and Yinan Su},
   doi = {10.1016/j.jfineco.2019.05.001},
   issn = {0304405X},
   issue = {3},
   journal = {Journal of Financial Economics},
   keywords = {Anomaly,BARRA,Conditional betas,Cross section of returns,Factor model,Latent factors,PCA},
   month = {12},
   pages = {501-524},
   publisher = {Elsevier B.V.},
   title = {Characteristics are covariances: A unified model of risk and return},
   volume = {134},
   year = {2019},
}
@article{Hou2018,
   abstract = {Most anomalies fail to hold up to currently acceptable standards for empirical finance. With microcaps mitigated via NYSE breakpoints and value-weighted returns, 65% of the 452 anomalies in our extensive data library, including 96% of the trading frictions category, cannot clear the single test hurdle of the absolute $t$-value of 1.96. Imposing the higher multiple test hurdle of 2.78 at the 5% significance level raises the failure rate to 82%. Even for replicated anomalies, their economic magnitudes are much smaller than originally reported. In all, capital markets are more efficient than previously recognized.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
   author = {Kewei Hou and Chen Xue and Lu Zhang},
   doi = {10.1093/rfs/hhy131},
   issn = {0893-9454},
   issue = {5},
   journal = {The Review of Financial Studies},
   month = {12},
   pages = {2019-2133},
   title = {Replicating Anomalies},
   volume = {33},
   url = {https://doi.org/10.1093/rfs/hhy131},
   year = {2018},
}
@article{Hou2014,
   abstract = {An empirical q-factor model consisting of the market factor, a size factor, an investment factor, and a profitability factor largely summarizes the cross section of average stock returns. A comprehensive examination of nearly 80 anomalies reveals that about one-half of the anomalies are insignificant in the broad cross section. More importantly, with a few exceptions, the q-factor model's performance is at least comparable to, and in many cases better than that of the Fama-French (1993) 3-factor model and the Carhart (1997) 4-factor model in capturing the remaining significant anomalies.},
   author = {Kewei Hou and Chen Xue and Lu Zhang},
   doi = {10.1093/rfs/hhu068},
   issn = {0893-9454},
   issue = {3},
   journal = {The Review of Financial Studies},
   month = {9},
   pages = {650-705},
   title = {Digesting Anomalies: An Investment Approach},
   volume = {28},
   url = {https://doi.org/10.1093/rfs/hhu068},
   year = {2014},
}
@article{Gu2020,
   abstract = {We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
   author = {Shihao Gu and Bryan Kelly and Dacheng Xiu},
   doi = {10.1093/rfs/hhaa009},
   issn = {0893-9454},
   issue = {5},
   journal = {The Review of Financial Studies},
   month = {2},
   pages = {2223-2273},
   title = {Empirical Asset Pricing via Machine Learning},
   volume = {33},
   url = {https://doi.org/10.1093/rfs/hhaa009},
   year = {2020},
}
@article{Zou2005,
   abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso. © 2005 Royal Statistical Society.},
   author = {Hui Zou and Trevor Hastie},
   doi = {10.1111/j.1467-9868.2005.00503.x},
   issn = {1369-7412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
   keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection},
   month = {4},
   pages = {301-320},
   title = {Regularization and variable selection via the elastic net},
   volume = {67},
   url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
   year = {2005},
}
@article{Carhart1997,
   abstract = {Using a sample free of survivor bias, I demonstrate that common factors in stock returns and investment expenses almost completely explain persistence in equity mutual funds' mean and risk-adjusted returns. Hendricks, Patel and Zeckhauser's (1993) "hot hands" result is mostly driven by the one-year momentum effect of Jegadeesh and Titman (1993), but individual funds do not earn higher returns from following the momentum strategy in stocks. The only significant persistence not explained is concentrated in strong underperformance by the worst-return mutual funds. The results do not support the existence of skilled or informed mutual fund portfolio managers.},
   author = {Mark M. Carhart},
   doi = {10.1111/j.1540-6261.1997.tb03808.x},
   issn = {00221082},
   issue = {1},
   journal = {The Journal of Finance},
   month = {3},
   pages = {57-82},
   publisher = {Blackwell Publishing Inc.},
   title = {On Persistence in Mutual Fund Performance},
   volume = {52},
   url = {http://doi.wiley.com/10.1111/j.1540-6261.1997.tb03808.x},
   year = {1997},
}
@article{Fama2015,
   abstract = {A five-factor model directed at capturing the size, value, profitability, and investment patterns in average stock returns performs better than the three-factor model of Fama and French (FF, 1993). The five-factor model's main problem is its failure to capture the low average returns on small stocks whose returns behave like those of firms that invest a lot despite low profitability. The model's performance is not sensitive to the way its factors are defined. With the addition of profitability and investment factors, the value factor of the FF three-factor model becomes redundant for describing average returns in the sample we examine.},
   author = {Eugene F. Fama and Kenneth R. French},
   doi = {10.1016/j.jfineco.2014.10.010},
   issn = {0304405X},
   issue = {1},
   journal = {Journal of Financial Economics},
   keywords = {Asset pricing model,Dividend discount model,Factor model,Investment,Profitability},
   month = {4},
   pages = {1-22},
   publisher = {Elsevier},
   title = {A five-factor asset pricing model},
   volume = {116},
   year = {2015},
}
@article{DeMiguel2017,
   abstract = {We investigate how many characteristics matter jointly for an investor who cares not only about average returns but also about portfolio risk and transaction costs.  Our main finding is that transaction costs significantly increase the dimension of the cross section of stock returns. While in the absence of transaction costs only a small number of characteristics--about six--are significant, in the presence of transaction costs this number _increases_ to 15. The explanation is that, as we show analytically and empirically, combining characteristics helps to substantially reduce transaction costs because the trades in the underlying stocks required to rebalance different characteristics net out. Our work demonstrates that transaction costs provide an economic rationale to consider a larger number of characteristics than that considered in prominent asset-pricing models.},
   author = {Victor DeMiguel and Alberto Martin-Utrera and Francisco J. Nogales and Raman Uppal},
   doi = {10.2139/ssrn.2912819},
   journal = {SSRN Electronic Journal},
   keywords = {cross section of stock returns,factor zoo,trading diversification},
   month = {6},
   publisher = {Elsevier BV},
   title = {A Transaction-Cost Perspective on the Multitude of Firm Characteristics},
   year = {2017},
}
@article{FAMA2008,
   abstract = {The anomalous returns associated with net stock issues, accruals, and momentum are pervasive; they show up in all size groups (micro, small, and big) in cross-section regressions, and they are also strong in sorts, at least in the extremes. The asset growth and profitability anomalies are less robust. There is an asset growth anomaly in average returns on microcaps and small stocks, but it is absent for big stocks. Among profitable firms, higher profitability tends to be associated with abnormally high returns, but there is little evidence that unprofitable firms have unusually low returns. © 2008 The American Finance Association.},
   author = {EUGENE F. FAMA and KENNETH R. FRENCH},
   doi = {10.1111/j.1540-6261.2008.01371.x},
   issn = {00221082},
   issue = {4},
   journal = {The Journal of Finance},
   month = {8},
   pages = {1653-1678},
   publisher = {John Wiley & Sons, Ltd},
   title = {Dissecting Anomalies},
   volume = {63},
   url = {http://doi.wiley.com/10.1111/j.1540-6261.2008.01371.x},
   year = {2008},
}
@article{Freyberger2020,
   abstract = {We propose a nonparametric method to study which characteristics provide incremental information for the cross-section of expected returns. We use the adaptive group LASSO to select characteristics and to estimate how selected characteristics affect expected returns nonparametrically. Our method can handle a large number of characteristics and allows for a flexible functional form. Our implementation is insensitive to outliers. Many of the previously identified return predictors don’t provide incremental information for expected returns, and nonlinearities are important. We study our method’s properties in simulations and find large improvements in both model selection and prediction compared to alternative selection methods.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
   author = {Joachim Freyberger and Andreas Neuhierl and Michael Weber},
   doi = {10.1093/rfs/hhz123},
   issn = {0893-9454},
   issue = {5},
   journal = {The Review of Financial Studies},
   month = {4},
   pages = {2326-2377},
   title = {Dissecting Characteristics Nonparametrically},
   volume = {33},
   url = {https://doi.org/10.1093/rfs/hhz123},
   year = {2020},
}
@article{Huang2010,
   abstract = {We consider a nonparametric additive model of a conditional mean function in which the number of variables and additive components may be larger than the sample size but the number of nonzero additive components is "small" relative to the sample size. The statistical problem is to determine which additive components are nonzero. The additive components are approximated by truncated series expansions with B-spline bases. With this approximation, the problem of component selection becomes that of selecting the groups of coefficients in the expansion. We apply the adaptive group Lasso to select nonzero components, using the group Lasso to obtain an initial estimator and reduce the dimension of the problem. We give conditions under which the group Lasso selects a model whose number of components is comparable with the underlying model, and the adaptive group Lasso selects the nonzero components correctly with probability approaching one as the sample size increases and achieves the optimal rate of convergence. The results of Monte Carlo experiments show that the adaptive group Lasso procedure works well with samples of moderate size. A data example is used to illustrate the application of the proposed method. © Institute of Mathematical Statistics, 2010.},
   author = {Jian Huang and Joel L. Horowitz and Fengrong Wei},
   doi = {10.1214/09-AOS781},
   issn = {00905364},
   issue = {4},
   journal = {Annals of Statistics},
   keywords = {Adaptive group lasso,Component selection,High-dimensional data,Nonparametric regression,Selection consistency},
   month = {8},
   pages = {2282-2313},
   publisher = {NIH Public Access},
   title = {Variable selection in nonparametric additive models},
   volume = {38},
   year = {2010},
}
@article{Lettau2020,
   abstract = {We develop an estimator for latent factors in a large-dimensional panel of financial data that can explain expected excess returns. Statistical factor analysis based on Principal Component Analysis (PCA) has problems identifying factors with a small variance that are important for asset pricing. We generalize PCA with a penalty term accounting for the pricing error in expected returns. Our estimator searches for factors that can explain both the expected return and covariance structure. We derive the statistical properties of the new estimator and show that our estimator can find asset-pricing factors, which cannot be detected with PCA, even if a large amount of data is available. Applying the approach to portfolio data we find factors with Sharpe-ratios more than twice as large as those based on conventional PCA and with smaller pricing errors.},
   author = {Martin Lettau and Markus Pelger},
   doi = {10.1016/j.jeconom.2019.08.012},
   issn = {18726895},
   journal = {Journal of Econometrics},
   keywords = {Anomalies,Cross section of returns,Expected returns,High-dimensional data,Latent factors,PCA,Weak factors},
   month = {2},
   publisher = {Elsevier Ltd},
   title = {Estimating latent asset-pricing factors},
   year = {2020},
}
@article{Belloni2014,
   author = {Alexandre Belloni and Victor Chernozhukov and Christian Hansen},
   doi = {10.1093/restud/rdt044},
   issn = {0034-6527},
   issue = {2},
   journal = {The Review of Economic Studies},
   month = {4},
   pages = {608-650},
   publisher = {Oxford Academic},
   title = {Inference on Treatment Effects after Selection among High-Dimensional Controls},
   volume = {81},
   year = {2014},
}
@article{Rapach2013,
   abstract = {We investigate lead-lag relationships among monthly country stock returns and identify a leading role for the United States: lagged U.S. returns significantly predict returns in numerous non-U.S. industrialized countries, while lagged non-U.S. returns display limited predictive ability with respect to U.S. returns. We estimate a news-diffusion model, and the results indicate that return shocks arising in the United States are only fully reflected in equity prices outside of the United States with a lag, consistent with a gradual information diffusion explanation of the predictive power of lagged U.S. returns. © 2013 the American Finance Association.},
   author = {David E. Rapach and Jack K. Strauss and Guofu Zhou},
   doi = {10.1111/jofi.12041},
   issn = {00221082},
   issue = {4},
   journal = {The Journal of Finance},
   month = {8},
   pages = {1633-1662},
   publisher = {John Wiley & Sons, Ltd},
   title = {International Stock Return Predictability: What Is the Role of the United States?},
   volume = {68},
   url = {http://doi.wiley.com/10.1111/jofi.12041},
   year = {2013},
}
@book{Cochrane2005,
   author = {John Cochrane},
   keywords = {Asset Pricing,Capital Market Theory,Economics,Financial Engineering,Mathematical Finance},
   pages = {xvii-xvii},
   title = {Asset pricing},
   year = {2005},
}
@article{Bai2008,
   abstract = {This paper studies two refinements to the method of factor forecasting. First, we consider the method of quadratic principal components that allows the link function between the predictors and the factors to be non-linear. Second, the factors used in the forecasting equation are estimated in a way to take into account that the goal is to forecast a specific series. This is accomplished by applying the method of principal components to 'targeted predictors' selected using hard and soft thresholding rules. Our three main findings can be summarized as follows. First, we find improvements at all forecast horizons over the current diffusion index forecasts by estimating the factors using fewer but informative predictors. Allowing for non-linearity often leads to additional gains. Second, forecasting the volatile one month ahead inflation warrants a high degree of targeting to screen out the noisy predictors. A handful of variables, notably relating to housing starts and interest rates, are found to have systematic predictive power for inflation at all horizons. Third, the targeted predictors selected by both soft and hard thresholding changes with the forecast horizon and the sample period. Holding the set of predictors fixed as is the current practice of factor forecasting is unnecessarily restrictive. © 2008 Elsevier B.V. All rights reserved.},
   author = {Jushan Bai and Serena Ng},
   doi = {10.1016/j.jeconom.2008.08.010},
   issn = {03044076},
   issue = {2},
   journal = {Journal of Econometrics},
   keywords = {Diffusion index,Factor models,Hard thresholding,LARS,LASSO},
   month = {10},
   pages = {304-317},
   publisher = {North-Holland},
   title = {Forecasting economic time series using targeted predictors},
   volume = {146},
   year = {2008},
}
@article{Bailey2019,
   abstract = {In this paper, we focus on estimating the degree of cross-sectional dependence in the error terms of a classical panel data regression model. For this purpose we propose an estimator of the exponent of cross-sectional dependence denoted by α, which is based on the number of non-zero pair-wise cross correlations of these errors. We prove that our estimator, α~ , is consistent and derive the rate at which it approaches its true value. We also propose a resampling procedure for the construction of confidence bounds around the estimator of α. We evaluate the finite sample properties of the proposed estimator by use of a Monte Carlo simulation study. The numerical results are encouraging and supportive of the theoretical findings. Finally, we undertake an empirical investigation of α for the errors of the CAPM model and its Fama-French extensions using 10-year rolling samples from S&P 500 securities over the period Sept 1989 - May 2018.},
   author = {Natalia Bailey and George Kapetanios and M. Hashem Pesaran},
   doi = {10.1007/s13571-019-00196-9},
   issn = {09768394},
   issue = {1},
   journal = {Sankhya B},
   keywords = {C21,C32,CAPM and Fama-French factors,Cross-sectional averages,Cross-sectional dependence,Pair-wise correlations,Weak and strong factor models},
   month = {9},
   pages = {46-102},
   publisher = {Springer},
   title = {Exponent of Cross-sectional Dependence for Residuals},
   volume = {81},
   year = {2019},
}
@article{Feng2020,
   abstract = {We propose a model selection method to systematically evaluate the contribution to asset pricing of any new factor, above and beyond what a high-dimensional set of existing factors explains. Our methodology accounts for model selection mistakes that produce a bias due to omitted variables, unlike standard approaches that assume perfect variable selection. We apply our procedure to a set of factors recently discovered in the literature. While most of these new factors are shown to be redundant relative to the existing factors, a few have statistically significant explanatory power beyond the hundreds of factors proposed in the past.},
   author = {Guanhao Feng and Stefano Giglio and Dacheng Xiu},
   doi = {10.1111/jofi.12883},
   issn = {0022-1082},
   issue = {3},
   journal = {The Journal of Finance},
   month = {6},
   pages = {1327-1370},
   publisher = {Blackwell Publishing Ltd},
   title = {Taming the Factor Zoo: A Test of New Factors},
   volume = {75},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.12883},
   year = {2020},
}
@article{Fama2018,
   abstract = {Our goal is to develop insights about the maximum squared Sharpe ratio for model factors as a metric for ranking asset pricing models. We consider nested and non-nested models. The nested models are the capital asset pricing model, the three-factor model of Fama and French (1993), the five-factor extension in Fama and French (2015), and a six-factor model that adds a momentum factor. The non-nested models examine three issues about factor choice in the six-factor model: (1) cash profitability versus operating profitability as the variable used to construct profitability factors, (2) long-short spread factors versus excess return factors, and (3) factors that use small or big stocks versus factors that use both.},
   author = {Eugene F. Fama and Kenneth R. French},
   doi = {10.1016/j.jfineco.2018.02.012},
   issn = {0304405X},
   issue = {2},
   journal = {Journal of Financial Economics},
   keywords = {Asset pricing tests,Factor model,Max squared Sharpe ratio,Sharpe ratio},
   month = {5},
   pages = {234-252},
   publisher = {Elsevier B.V.},
   title = {Choosing factors},
   volume = {128},
   year = {2018},
}
@article{Gospodinov2014,
   abstract = {This paper shows that in misspecified models with risk factors that are uncorrelated with the test asset returns, the conventional inference methods tend to erroneously conclude, with high probability, that these factors are priced. Our proposed model selection procedure, which is robust to identification failure and potential model misspecification, restores the standard inference and proves to be effective in eliminating factors that do not improve the model's pricing ability. Applying our methodology to several popular asset-pricing models suggests that only the market and book-to-market factors appear to be priced, while the statistical evidence on the pricing ability of many macroeconomic factors is rather weak. © 2014 The Author.},
   author = {Nikolay Gospodinov and Raymond Kan and Cesare Robotti},
   doi = {10.1093/rfs/hht135},
   issn = {14657368},
   issue = {7},
   journal = {Review of Financial Studies},
   month = {7},
   pages = {2139-2170},
   publisher = {Oxford University Press},
   title = {Misspecification-robust inference in linear asset-pricing models with irrelevant risk factors},
   volume = {27},
   url = {https://academic.oup.com/rfs/article/27/7/2139/1578148},
   year = {2014},
}
