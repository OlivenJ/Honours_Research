\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{mathptmx}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage[margin=2.5cm]{geometry}
\usepackage[title]{appendix}
\usepackage{bm}
\usepackage{tcolorbox}
\usepackage{apacite}
\onehalfspacing
\usepackage{lineno}
\linenumbers
\usepackage{diagbox}

\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{fontspec}
\setmainfont{Times New Roman}
\addtolength{\jot}{0.5em}
\linespread{1.5}
		
		\begin{document}
\section{Factor Strength}\label{strength}
The concept of factor strength employed by this project comes from \citeA{Bailey2020}, and it was first introduced by \citeA{Bailey2016}.
They defined the strength of factor from prospect of the cross-section dependences of large panel and connect it to the pervasiveness of the factor, which is captured by the factor loadings.
The method present in the initial paper focusing the estimation from unobserved factor.
In a latter paper, \citeA{BKP2019} extended the method from estimating the strength from the residuals, and further developed into the method, which focus on the observed factors we employed in this project \cite{Bailey2020}.

		\subsection{Definition}\label{definiton}

% Capital Asset Pricing Model (CAPM) is the benchmark for pricing the systematic risk of a portfolio. 
Consider the following multi-factor model for n different cross-section units and T observations with k  factors.

\[  x_{it} = a_{t}+  \sum_{j=1}^{k}\beta_{ij}f_{jt} + \varepsilon_{it} \tag{1}\label{definition_model} \]
In the left-hand side, we have $x_{it}$ denotes the cross-section unit i at time t, where $i = 1, 2,3, \cdots, n$ and $t = 1,2,3, \cdots T$.  
In the other hand, $a_{i}$ is the constant term.
$f_{jt}$ of $j = 1, 2, 3\cdots k$ is factors included in the model, and $\beta_{ij}$ is the corresponding factor loading.
$\varepsilon_{it}$ is the stochastic error term.

The factor strength is relates to how many non-zero loadings correspond to a factor.
More precisely, for a factor $f_{jt}$ with n different factor loading $\beta_{ij}$, we assume that:
\begin{align*}
|\beta_{j}| &> 0\quad i = 1, 2,  \dots, [n^{\alpha_j}]\\
|\beta_{j}| &= 0 \quad i = [n^{\alpha_j}] + 1, [n^{\alpha_j}] +2 ,\dots, n
\end{align*}
The $\alpha_j$ represents strength of factor $f_{jt}$ and $\alpha_j \in [0,1]$.
If factor has strength $\alpha_j$, we will assume that the first $[n^{\alpha_j}]$ loadings are all different from zero, and here $[\cdot] $  is defined as integral operator, which will only take the integral part of inside value.% which indicates that those factors can capture the dynamic behind the cross-section unit effectively.
The rest $n - [n^{\alpha_j}]$ terms are all equal to zero. % which means the factors can not provides any information of interest of  those $n - [n^\alpha_j]$  units.
Assume for a factor which has strength $\alpha = 1$, the factor's loadings will be non-zero for all cross-section units.
We will refer such factor as strong factor.
And if we have factor strength $\alpha = 0$, it means that the factor has all factor loadings equal to zero, and we will describe such factor as weak factor \cite{Bailey2016}.
For any factor with strength in [0.5, 1], we will refer such factor as semi-strong factor.
In general term, the more non-zero loading a factor has, the stronger the factor's strength is. 


%The idea of factor loading is trying to describe for a factor, how many non-zero loadings it can generate through a group of cross-section units.


%The factor strength$\alpha_j$ of factor $f_{jt}$ as is dependent on how many significant factor loadings it can generate.
%The more statistically non-zero loading a factor can generate, the stronger the factor is.
%When we have n assets, there are $[n^{\alpha}]$ are not zero, $[\cdot]$ denotes the integer number operator, which will take the integer part of number inside. 
%For factor $f_{j}$ with loading $\beta_j$, we assume it should have:
%\begin{align*}
%|\beta_{j}| &> c_p(n)\quad i = 1, 2,  \dots, [n^{\alpha_j}]\\
%|\beta_{j}| &= 0 \quad i = [n^{\alpha_j}] + 1, [n^{\alpha_j}] +2 ,\dots, n
%\end{align*}
%The factor loadings of first $[n^{\alpha_j}]$ terms are all bigger than critical value $c_p(n)$, this indicates that those factors are all significantly different from zero.
%Then the rest $n - [n^{\alpha_j}]$ term are equal to zero, which means the factors can not pricing the risk for those $n - [n^\alpha_j]$ assets.
%For a factor has strength $\alpha = 1$,  factor loading will be significant for every assets at every time. 
%And if we have factor strength $\alpha = 0$, it means that the factor cannot generate any loading different from zero, in other words the factor can not pricing risk of any %assets.

	\subsection{Estimation}\label{estimation}
To estimate the strength $\alpha_j$, \citeA{Bailey2020} provides following estimation.

To begin with, we consider a single-factor model with only factor named $f_t$. 
 $\beta_{i}$ is the factor loading of unit i.
$v_{it}$ is the stochastic error term.

\[  x_{it} = a_{i} +  \beta_{i}f_{t} + v_{it} \tag{2} \label{estimation_model}\]

Assume we have n different units and T observations for each unit: $i = 1, 2, 3, \cdots, n$ and $t = 1,2,3, \cdots T$.
Running the OLS regression for each $i = 1,2,3\cdots, n$, we obtain:
\[   x_{it} = \hat{a}_{iT} +  \hat{\beta}_{iT}f_{t} + \hat{v}_{it}  \]

For every factor loading $\hat{\beta}_{iT}$, we can examining their significance by constructing a t-test.
The t-test statistic will be $t_{iT} = \frac{\hat{\beta}_{iT} - 0}{\hat{\sigma}_{iT}}$.  
Then the test statistic for the corresponding $\hat{\beta}_i$ will be:

\[t_{i T}=\frac{\left(\mathbf{f}^{\prime} \mathbf{M}_{\tau} \mathbf{f}\right)^{1 / 2} \hat{\beta}_{i T}}{\hat{\sigma}_{i T}}=\frac{\left(\mathbf{f}^{\prime} \mathbf{M}_{\tau} \mathbf{f}\right)^{-1 / 2}\left(\mathbf{f}^{\prime} \mathbf{M}_{\tau} \mathbf{x}_{i}\right)}{\hat{\sigma}_{i T}} \tag{3} \label{test_statistic} \]
Here, the $\mathbf{M}_{\tau} = \mathbf{I}_T - T^{-1}\mathbf{\tau}\mathbf{\tau^\prime}$, and the $\mathbf{\tau}$ is a $T\times 1$ vector with every elements equals to 1.
$\mathbf{f}$ and $\mathbf{x_i}$ are two vectors with: $\mathbf{f} = (f_1, f_2 \cdots, f_T)^{\prime}$   $\mathbf{x_i} = (x_{i1}, x_{i2}, \cdots, x_{iT})$.
The denominator $\hat{\sigma}_{iT} = \frac{\sum_{i=1}^{T} \hat{v}^2_{it} }{T}$.

Using this test statistic, we can then define an indicator function as: $\ell_{i,n} := {\bf1}[|\beta_i|>0]$.
If the factor loading is none-zero, $\ell_{i,n} = 1$.
In practice, we use the $\hat{\ell}_{i,nT} = := {\bf1}[|t_{it}|>c_p(n)]$
Here, if the t-statistic $t_{iT}$ is greater than critical value $c_p(n)$,  $\hat{\ell}_{i,n} = 1$, otherwise $\hat{\ell}_{i,n} = 0$
In other word, we are counting how many $\hat{\beta}_{iT}$ are significant.
With the indicator function, we then defined $\hat{\pi}_{nT}$ as the fraction of significant factor loading amount to the total factor loadings:

\[  \hat{\pi}_{nT} = \frac{\sum_{i=1}^n \hat{\ell}_{i,nT}}{n} \tag{4} \label{pi_function} \]


In term of the critical value $c_p(n)$, rather than use the traditional critical value from student-t distribution $\Phi^{-1}(1-\frac{P}{2})$, we use:
%represent the critical value of a test with test size $p$. 
%The critical value is calculated by:

\[   c_p(n) = \Phi^{-1}(1 - \frac{p}{2n^\delta})   \tag{5} \label{critical_value_function} \]

Suggested by \citeA{Bailey2019}, here, $\Phi^{-1}(\cdot)$ is the inverse cumulative distribution function of a standard normal distribution, P is the size of the test, and $\delta$ is a non-negative value represent the critical value exponent. 
In the scenario of cross-section unit's dimension excess the time observation's dimension, this critical value estimation has been proved that 


This estimated critical value, has been showed that, under both Gaussian and non-Gaussian,  can provides a true positive rate tend to unit with probability one, meanwhile the type-one error rate converges to zero with probability one.
%The traditional method to calculate critical value $\Phi^{-1}(1 - \frac{p}{2}) $ does not take multiple-test problem into the consideration.
%Here, by adding the $n^\delta$ term into the denominator, the new critical value has adjusted the multiple-test problem. 
%Therefore, we will use this critical value $c_p(n)$ to compare with the test statistic to justify does the factor loading is significantly different from zero.
%One of the most commonly used adjustment for multiple testing problem is Bonferroni correction. 
%When $n$ as sample size goes to infinity, however, the Bonferroni correction can not yield satisfying asymptotic results since the $\frac{p}{2n^{\delta}} \to 0$ when $n \to \infty$. 
%Therefore, \citeA{Bailey2020} provides another adjustment with additional exponent $\delta$ to constrain the behaviour of $n$.
%In this case, the $\delta$ is the critical value of standard t-test with same significant level and degree of freedom. 

After obtain the $\hat{\pi}_{nT}$, we can use the following formula provided by \citeA{Bailey2020} to estimate our strength indicator $\alpha_j$:
\[ \hat{\alpha} = \begin{cases}
1+\frac{\ln(\hat{\pi}_{nT})}{\ln n} & \text{if}\; \hat{\pi}_{nT} > 0,\\
0, & \text{if}\; \hat{\pi}_{nT} = 0.
\end{cases} \]

Whenever we have $\hat{\pi}_{nT}$, the estimated $\hat{\alpha}$ will be equal to zero. 
From the estimation, we can find out that $\hat{\alpha} \in [0,1]$

%\subsection{Extension to Multi-factor Model}
%This estimation method can be easily extended into a multi-factor scenario.
%Consider the similar model as model(\ref{2CAPM}), but here we write the market factor $\r_{mt} - r_{ft}$ as a risk factor:

%\[  x_{it} = a_i + \sum_{j = 1}^{k+1}\beta_{ij}f_{jt} + v_{it} =a_i + \mathbf{\beta_{ij}f_{jt}} + v_{it}, \text{for}\;\;i = 1,2,3, \cdots, n, \text{and}\;\; t = 1,2,3, \cdots, T\]

This estimation can also be extended into a multi-factor set up.


%+==================+==================+==================+===============&
%+==================+==================+==================+===============&

	\section{Monte Carlo Design}\label{MC}
	\subsection{Design}
In order to study the limited sample property of factor strength $\alpha_j$, we designed a Monte Carlo simulation.
Through the simulation, we compare the property of the factor strength in different settings.
Since we will apply the factor strength under the scenario of CAPM model, we consider the following data generating process (DGP): a multi-factor CAPM model.

\[ x_{it} = q_1({r_{mt}} - r_f) + q_2( \sum_{j=1}^k\beta_{ij}f_{jt}) +\epsilon_{it}  \]


In the simulation, we consider a dataset has $i = 1, 2,\dots, n$ different cross-section units, with $t= 1, 2,\dots, T$ different observations. 
$x_{it}$ is the cross-section return of different asset.
$f_{jt}$ represents different risk factors, and the corresponding  $\beta_{ij}$ are the factor loadings.
We use $r_{mt} - r_{ft}$ to denotes the market factor.
The $r_{mt}$ is the average market return and $r_{ft}$represent the risk free return.
By assumption, the market factor will has strength equals to one all the time, so we consider the market factor as factor $f_{m}$ which has strength $\alpha_m = 1$.
$\varepsilon_{it}$ is the stochastic error term.
Therefore, the simulation model can be simplified as:

\[ x_{it} = q_1(f_{mt}) + q_2( \sum_{j=1}^k\beta_{ij}f_{jt}) +\epsilon_{it}  \]

$q_1(\cdot)$ and $q_2(\cdot)$ are two different functions represent the unknown mechanism of market factor and other risk factors in pricing asset risk.
In the classical CAPM model and it's multi-factor extensions, for example the three factor model introduced by \citeA{Fama1992}, both $q_1$ and $q_2$ are linear.

 For each factor, we assume they follow a multinomial distribution with mean zero and a $k\times k$ variance-covariance matrix $\Sigma$. 
\begin{align*}
\mathbf{f_t} = \begin{pmatrix}
f_{i,t}\\f_{2,t}\\\vdots\\f_{k,t}
\end{pmatrix} \sim MVN(\mathbf{0}, \Sigma) \quad
 \Sigma := 
\begin{pmatrix}
\sigma^2_{f_1}, & \rho_{12}\sigma_{f1}\sigma_{f2} &\cdots  & \rho_{1k}\sigma_{f1}\sigma_{fk}\\
\rho_{12}\sigma_{f2}\sigma_{f1}, & \sigma^2_{f2} &\cdots  & \rho_{2k}\sigma_{f2}\sigma_{fk}\\
\vdots & \vdots & \ddots & \vdots \\
\rho_{1k}\sigma_{fk}\sigma_{f1}, & \rho_{k2}\sigma_{fk}\sigma_{f2} &\cdots  & \sigma^2_{fk}\\
\end{pmatrix}
\end{align*}
The diagonal of matrix $\Sigma$ indicates the variance of each factor, and the rest represent the correlation among all $k$ factors.

\subsection{Baseline Experiment}\label{base}
Follow the general model above, we assume both $q_1(\cdot)$ and $q_2(\cdot)$ are linear function:
\begin{align*}
q_1({f_{mt}}) &= a_{it} +\beta_{im} f_{mt}\\
q_2(\sum_{j = 1}^{k}\beta_{ij}f_{jt}) &=\sum_{j = 1}^{k}\beta_{ij}f_{jt}
\end{align*}
Therefore, if we include the market factor with other risk factors together, the model can be simplified as:
	\[   x_{it} = a_{it} + \sum_{j = 1}^{k+1}  \beta_{ij}f_{jt} +\epsilon_{it}  \tag{6} \label{simplified_multi}  \]
And in this first baseline experiment, we will  use the single factor model as:
\[  x_{it} = a_{it} +  \beta_{i1}f_{1t} +\epsilon_{it} \tag{7} \label{singlefactor}  \]
Through the simulations, we will control the underlying true strength of factor 
%The constant $a_{it}$ is generated from a uniform distribution $\mathnormal{U}[-0.5, 0.5]$.
%Since we have n different assets, and for each asset we have T different observations, we will construct a $n\times T$ matrix to store all constant $a_{it}$
%	\[   \mathbf{a} = \begin{pmatrix}
%a_{11}& a_{12}&\cdots& a_{1T}\\
%a_{21}& a_{22}&\cdots& a_{2T}\\
%\vdots,& \vdots&\ddots&\vdots\\
%a_{k1}& a_{k2}&\cdots& a_{kT}\\
%	\end{pmatrix} \]
%$\beta_{ij}$ is the factor loading, and $f_{ij}$ is factor with strength $\alpha_{j}$. 
%Notice that here we have included the market factor $r_{mi - r_f}$ as $f_{i1}$, which has strength equals to 1.

To generate factor loadings and asset's return, we first generate the constant term $a_{it}$ which has a uniform distribution from -0.5 to 0.5, $a_{it} \sim \mathnormal{U}[-0.5,0.5]$.
%For each asset we generate T different time observations, so we will obtain $n\times T$ different constant term.
Then, in this baseline design, we assume the error term follow a standard normal distribution $\epsilon_{it}\sim N(0,1)$.
Next, we set up the true factor strength $\alpha$.
Through the whole simulation, we will assign the strength with different value $\alpha = \{0.5, 0.7, 0.9, 1\}$, and since in this baseline design we only contain one factor, the only factor's strength will be selected from the above set. 
After having the factor strength, we can calculate for each factor, how many loadings should be different from zero.
From the section (\ref{definiton}), we assume that for any factor with strength $\alpha_j$, the factor is supposed to generate $[n^{\alpha_j}]$ non-zero factor loadings, and $n- [n^{\alpha_j}]$ zero loadings.
Therefore, we can calculate the $n - [n^{\alpha_j}]$.

From the previous section, we assume factors will follow a multinomial standard distribution with mean zero and variance $\Sigma$.
This means that for each factors, they should follow a normal distribution.
In this baseline design, we only contain one factor, and this factor will generate form standard error distribution.

After that, we will generate the factor loadings from a uniform distribution.
To make sure every factor loading is sufficiently larger than 0, we set the expected value of those loadings  $\mu_{\beta} = 0.71$, $\beta_{i1} \sim \mathnormal{IIDU}(\mu_{\beta}-0.2,\, \mu_{\beta}+0.2)$.
Then we randomly assign $n - [n^\alpha]$ factor loadings as zero, to reflect the fact that only $[n^\alpha]$ factor loadings are non-zero.


For this experiment, we construct the hypothesis test base on the null hypothesis $H_O:\beta_{i1} = 0$ against the alternative hypothesis $H_1:\beta_{i1}\neq0$.
The  test statistic and critical value are from equation (\ref{test_statistic}) and equation (\ref{critical_value_function}).
We consider two-sided tests, with size 0.05.
Therefore, the corresponding critical value for such t-test will be $\delta = 1.96$

%For the test size p and critical value exponent $\delta$, we set $p = 0.05$ and $\delta = 1.96$.


After generate constant term, factor, factor loading, and the error term, we can calculate the simulated asset's return by using the equation (\ref{singlefactor}).
With the return and factors, we can re-calculate the factors loading and use the estimation method discussed in section \ref{estimation}.

%First we generate a whole factor loadings vector $\mathbf{\beta_i} = (\beta_{i1}, \beta_{i2} \cdots, \beta_{ik+1})$,
%All elements of the vector follows $\mathnormal{IIDU}(\mu_{\beta} -0.2, \mu_{\beta} + 0.2)$. 
%The $\mu_{\beta}$ has been equalled to 0.71 to ensure all values apart from zero. 
%After generating the vector, we randomly selected $[n^{\alpha_{j}}]$ elements from $\mathbf{\beta_i}$ to keep their value and set the other elements value to zero. 
%This step ensures the loading reflects the strength of each factor. 
%For the stochastic error term, in this baseline design, we assume it follows a Standard Gaussian distribution, but we can easily extend it into a more complex form.

%Follow the same idea, we also construct a two factor model:
%\[   r_{it}-r_{ft} = a_{it} + \beta_{im} (r_{mt} - r_{ft})  + \sum_{j = 1}^k\beta_{ij}f_{jt} +\epsilon_{it}   \]
%Here the $r_{mt} - r_{ft}$ is the market factor which assumably  has strength $\alpha_{m} = 1$. 
%${\beta_m}$ is the market factor loading as a vector with all elements different from zero. 


\subsection{Two factor experiment}
Follow the similar idea as baseline design, we can easily extend the DGP into multi-factor form. 
We derive a two-factor model from the model  (\ref{simplified_multi}).
	\[   x_{it} = a_{it} + \beta_{i1}f_{1t} + \beta_{i2}f_{2t}+\epsilon_{it}  \tag{8}  \]
Here $\mathbf{f}_t = (f_{1t}, f_{2t})^{\prime}$ are two different factors generate from multivariate normal distribution with mean zero and variance $\Sigma$.
In this simulation, we assume two factors are independent with each other and both of them have variance equals to one, the variance-covariance matrix $\Sigma$ of factors $\mathbf{f_t}$ will be:
\begin{align*}
\Sigma_{\mathbf{f_t}} := 
\begin{pmatrix}
1 &0\\
0 & 1 \\
\end{pmatrix}
\end{align*}
Besides, for the factor $f_{1t}$, we assign it as the market factor, which indicates that the factor strength $\alpha_1$ will be unit.
And all factor loading generates from this factor will be different from zero.
For the rest of the variables, we follow the same procedure as the baseline experiment.

In purpose of Monte Carlo Simulation, we consider the different combinations of T and n with $T = \{120, 240, 360\}$, $n =\{100, 300, 500\} $.
The market factor, if included in the experiment, will have strength $\alpha_m = 1$ all the time, and the strength of the other factor will be $\alpha_{x} = \{0.5, 0.7, 0.9,1\}$. For every setting, we will replicate 500 times independently, all the constant $a_{it}$ and loading $\beta_i$ will be re-generated for each replication.
To exam the goodness of estimation, we calculate the bias between our true underneath factor strength $\alpha$ and the estimated strength $\hat{\alpha}$ as $ bias = |\alpha - \hat{\alpha}|$. 
We also use the bias to calculate the Mean Square Error (MSE).
To calculate the MSE, we will collect the bias for each replication, and then use the formula: 
\[ MSE =\frac{1}{n}\sum_{i=1}^{500}(bias_i)^2 \]


\subsection{Monte Carlo Discoveries}
We report the results in Table (\ref{simutable1}) and Table (\ref{simutable2}) for baseline experiment and two-factor experiment respectively.
The Table (\ref{simutable1}) shows the bias and MSE for different $\alpha$ and different (n, T) combinations under the single factor setting.
Because the Table (\ref{simutable2}) is the result of two factor 
Table (\ref{simutable2}) shows the bias and MSE for different $\alpha_2$ with $\alpha_1 = 1$ under different (n, T) combinations.
The two tables shows very similar results.
The estimation method we applied tend to over-estimate the strength when the true strength is relatively weak.
The bias is around 0.2 when the true underlying factor strength is 0.5.
Such bias, however, decrease gradually with $\alpha$ rise.
When the strength increase to 0.7, the bias will decrease about 0.1 unit.
And when the true factor strength is 1,the strongest it can be, we find that the bias and MSE are all converge to zero under all sample size and time combinations. 

\newpage
\bibliographystyle{apacite}
\bibliography{thesis.bib}

\newpage
\appendix
\input{appendix.tex}


		\end{document}